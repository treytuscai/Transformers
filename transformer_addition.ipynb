{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4161dcc7",
   "metadata": {},
   "source": [
    "Trey Tuscai and Gordon Doore\n",
    "\n",
    "Spring 2025\n",
    "\n",
    "CS 444: Deep Learning\n",
    "\n",
    "#### Project 4: Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90adbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use(['seaborn-v0_8-colorblind', 'seaborn-v0_8-darkgrid'])\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=4)\n",
    "\n",
    "# Automatically reload your external source code\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4e8b16",
   "metadata": {},
   "source": [
    "![Some fun](images/transformer2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6bece8",
   "metadata": {},
   "source": [
    "## Task 5: Create GPT networks\n",
    "\n",
    "With the Transformer Block and other network components implemented, let's build an actual transformer! We are implementing a transformer that follows the design of OpenAI's GPT line of neural networks. Therefore, the network class is called `GPT` (still inherits from `DeepNetwork` like usual). Specific GPT networks that you will construct will be child classes of `GPT` (much like `ResNet8` and `ResNet18` were child classes of `ResNet`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7c7f91",
   "metadata": {},
   "source": [
    "### 5a. Building `GPTPico1`\n",
    "\n",
    "Let's build a minimal GPT that consists of only one Transformer Block. This network, `GPTPico1`, has the following architecture:\n",
    "1. Embedding layer\n",
    "2. Positional encoding block.\n",
    "3. Transformer block (1x)\n",
    "4. Dense output layer\n",
    "\n",
    "Implement and test the following required methods:\n",
    "- `GPT`: constructor.\n",
    "- `GPT`: `__call__` method. Forward pass through the transformer.\n",
    "- `GPTPico1`: constructor. Assemble the net (*see above*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd0ad70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpts import GPT, GPTPico1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4b7428",
   "metadata": {},
   "source": [
    "#### Test: `loss` (temporal cross-entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc07f558",
   "metadata": {},
   "outputs": [],
   "source": [
    "mygpt = GPT(seq_len=4, padding_char_enc=10)\n",
    "mygpt.loss_name = 'temporal_cross_entropy'\n",
    "\n",
    "tf.random.set_seed(1)\n",
    "y_pred = tf.random.uniform(shape=(5, 4, 12), maxval=1, dtype=tf.float32)\n",
    "y_true = tf.random.uniform(shape=(5, 4), maxval=11, dtype=tf.int32)\n",
    "\n",
    "# Test 1: no masking the padding char\n",
    "loss = mygpt.loss(y_pred, y_true, mask_padding_preds=False)\n",
    "print(f'When not masking out the padding char, your loss is {loss.numpy():.4f} and it should be 0.9513.')\n",
    "# Test 2: masking the padding char\n",
    "loss = mygpt.loss(y_pred, y_true)\n",
    "print(f'When masking out the padding char, your loss is {loss.numpy():.4f} and it should be 0.9442.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a658a3f6",
   "metadata": {},
   "source": [
    "#### Test: `GPTPico1` architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc6f304",
   "metadata": {},
   "outputs": [],
   "source": [
    "mypicogpt = GPTPico1(vocab_sz=9, seq_len=15, padding_char_enc=5)\n",
    "mypicogpt.compile(loss='temporal_cross_entropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a39dc3e",
   "metadata": {},
   "source": [
    "The above cell should print out:\n",
    "\n",
    "```\n",
    "---------------------------------------------------------------------------\n",
    "Dense layer output(output) shape: [1, 15, 9]\n",
    "TransformerBlock_0:\n",
    "\tTransformerBlock_0/MLP:\n",
    "\tDropout layer output(TransformerBlock_0/MLP/dropout) shape: [1, 15, 24]\n",
    "\tDense layer output(TransformerBlock_0/MLP/dense_1) shape: [1, 15, 24]\n",
    "\tDense layer output(TransformerBlock_0/MLP/dense_0) shape: [1, 15, 96]\n",
    "\tTransformerBlock_0/multihead_attention:\n",
    "\tDropout layer output(TransformerBlock_0/multihead_attention/dropout) shape: [1, 15, 24]\n",
    "\tDense layer output(TransformerBlock_0/multihead_attention/dense_1) shape: [1, 15, 24]\n",
    "\tTransformerBlock_0/multihead_attention/attention:\n",
    "\tDropout layer output(TransformerBlock_0/multihead_attention/attention/dropout) shape: [1, 4, 15, 15]\n",
    "\tTransformerBlock_0/multihead_attention/qkv_block:\n",
    "\tDense layer output(TransformerBlock_0/multihead_attention/qkv_block/dense_v) shape: [1, 15, 24]\n",
    "\tDense layer output(TransformerBlock_0/multihead_attention/qkv_block/dense_k) shape: [1, 15, 24]\n",
    "\tDense layer output(TransformerBlock_0/multihead_attention/qkv_block/dense_q) shape: [1, 15, 24]\n",
    "PositionalEncodingBlock:\n",
    "\tDropout layer output(PositionalEncodingBlock/dropout) shape: [1, 15, 24]\n",
    "\tPositional encoding layer output(PositionalEncodingBlock/positional_enc_layer) shape: [1, 15, 24]\n",
    "Embedding layer output(EmbeddingLayer) shape: [1, 15, 24]\n",
    "---------------------------------------------------------------------------\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516e45bc",
   "metadata": {},
   "source": [
    "### 5b. `GPTPico1` overfit test\n",
    "\n",
    "Let's verify that your `GPTPico1` works by overfitting a small amount of fake data.\n",
    "\n",
    "Running the following cell should result in a training loss of ~`0.03` by 500 training epochs. The training loss after 1 epoch should be ~`5.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04512aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)\n",
    "mypicogpt = GPTPico1(vocab_sz=100, seq_len=5, padding_char_enc=99)\n",
    "mypicogpt.compile(loss='temporal_cross_entropy')\n",
    "\n",
    "tf.random.set_seed(1)\n",
    "x_dev_test = tf.random.uniform(shape=(10, 5), maxval=100, dtype=tf.int32)\n",
    "y_dev_test = tf.random.uniform(shape=(10, 5), maxval=100, dtype=tf.int32)\n",
    "train_loss_hist, _, _, _ = mypicogpt.fit(x_dev_test, y_dev_test, x_dev_test, y_dev_test, batch_size=10, max_epochs=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc4f815",
   "metadata": {},
   "source": [
    "## Task 6: Train GPTs on the Addition Dataset\n",
    "\n",
    "In task, you will train small transformers on the Addition dataset, have the transformers generate the answers to addition problems, and analyze the properties of the trained transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004ba28c",
   "metadata": {},
   "source": [
    "### 6a. Train `GPTPico1` on a small amount of Addition Dataset expressions\n",
    "\n",
    "In the cell below, train `GPTPico1` on the first 25 samples from the Addition dataset. Use:\n",
    "- the training set as the validation set\n",
    "- the default random seed.\n",
    "- a patience of `15` (no learning rate decay).\n",
    "- a batch size of `25` (*batch gradient descent*).\n",
    "\n",
    "You should get a final training loss of less than `0.20`.\n",
    "\n",
    "Make a well-labeled plot showing the training loss over the course of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98adeb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from addition_dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fe29fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, _, _, char2ind_map = get_addition_dataset(N=25, val_prop=0.)\n",
    "vocab_sz = len(char2ind_map)\n",
    "seq_len = x_train.shape[1]\n",
    "\n",
    "\n",
    "print('First 5 expressions (encoded)')\n",
    "print(x_train[:5].numpy())\n",
    "print('First 5 target labels (encoded)')\n",
    "print(y_train[:5].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ac975d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d794106",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4d9c331",
   "metadata": {},
   "source": [
    "### 6b. Test `generate_sequence`\n",
    "\n",
    "This is the method that allows you to prompt your transformer and have it generate text that follows your prompt.\n",
    "\n",
    "Test it out in the cell below on your GPTPico1 network **trained** on the first 25 addition expressions (*the one that yielded a final training loss of less than `0.20` above*) by prompting it with the left-hand side of a single one of the 25  training samples.\n",
    "\n",
    "For example, if you prompt `'47+51='` (the 1st sample) it should output `98.` (or `'47+51=98.'` if you have chatbot-style character-by-character live printing turned on as it generates).\n",
    "\n",
    "Another example prompt is `'75+95='` (2nd sample), which should output `170.` (or `'75+95=170.'` if live printing is on).\n",
    "\n",
    "**Note:** The end character is set to `'.'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9144069e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind2char_map = make_ind2char_mapping(char2ind_map)\n",
    "\n",
    "\n",
    "answer = gpt_add.generate_sequence(prompt=prompt,\n",
    "                                   length=seq_len,\n",
    "                                   char2ind_map=char2ind_map,\n",
    "                                   ind2char_map=ind2char_map,\n",
    "                                   end_char='.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43ebf42",
   "metadata": {},
   "source": [
    "### 6c. Verify transformer overfits all 25 addition expressions\n",
    "\n",
    "Once you have the above `generate_sequence` test working, \"script\" your GPT to generate the answers to all 25 addition expressions in the current subset of the addition dataset. When you print out the generated answers, also print out the true answer so that you can visually scan the correctness of the outputs. Your `GPTPico1` should get all or the vast majority of additions correct.\n",
    "\n",
    "**Reminders:**\n",
    "- Make sure you set the end character to the appropriate char.\n",
    "- Use the `convert_int2str` and `split_sum_and_answer` methods that you wrote to prepare the prompts and expected answers for each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d776e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from addition_dataset import convert_int2str, split_sum_and_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cd9751",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33aa0bde",
   "metadata": {},
   "source": [
    "### 6d. Transformer learns to add\n",
    "\n",
    "In the cell below, train `GPTPico1` on an addition dataset with `25000` expressions the default configuration (e.g. 90/10 train/val split). Use default network and training hyperparameters except for:\n",
    "- batch size of `1024`.\n",
    "- patience of `15`.\n",
    "- learning rate patience of `5`.\n",
    "- at most `4` learning rate decays.\n",
    "- at most `1200` training epochs.\n",
    "\n",
    "Do the following after training your net:\n",
    "1. Create a well-labeled plot showing the training and validation loss over epochs.\n",
    "2. Print out of the generated answers for the 1st 50 **training AND validation set** samples separately. Each print out should show the prompt (left-hand side of `=`, including the `=`), the answer generated by the transformer, and the correct answer â€” all in a neat, easy to read format.\n",
    "\n",
    "*If everything is working as expected, you should be able to achieve a validation loss in the 0.9s*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55efb3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from addition_dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20273a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4706b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ba432a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350ce645",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_show = 50\n",
    "x_splits = [x_train, x_val]\n",
    "split_labels = ['train', 'val']\n",
    "\n",
    "for i in range(2):\n",
    "    x = x_splits[i]\n",
    "    split = split_labels[i]\n",
    "\n",
    "    print(50*'=')\n",
    "    print(split)\n",
    "    print(50*'=')\n",
    "    N = len(x)\n",
    "\n",
    "    ind2char_map = make_ind2char_mapping(char2ind_map)\n",
    "    x_str = convert_int2str(x_int=x.numpy(), ind2char_map=ind2char_map)\n",
    "    prompts, correct_answers = split_sum_and_answer(x_str)\n",
    "\n",
    "\n",
    "    for i in range(N_show):\n",
    "        curr_prompt = prompts[i]\n",
    "        curr_ans = correct_answers[i]\n",
    "        answer = gpt_add1.generate_sequence(prompt=curr_prompt,\n",
    "                                            length=seq_len,\n",
    "                                            char2ind_map=char2ind_map,\n",
    "                                            ind2char_map=ind2char_map,\n",
    "                                            end_char='.')\n",
    "        print('Correct answer is:', curr_ans)\n",
    "        print('---------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5959a8a2",
   "metadata": {},
   "source": [
    "### 6e. Questions\n",
    "\n",
    "**Question 4:** Look over the addition prompts and generated answers for mistakes.\n",
    "\n",
    "a. What types of mistakes do you spot?\n",
    "\n",
    "b. How close are the mistakes to the true answers?\n",
    "\n",
    "c. Why do you think the transformer is making the mistakes it makes?\n",
    "\n",
    "*If your transformer is not making mistakes, print out more prompts/answers or cut the training off a little earlier so the final validation loss is higher.*\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "**Question 5:** Have some fun and prompt your trained transformer with hand crafted addition prompt strings. Try to \"trick\" your transformer:\n",
    "\n",
    "a. using valid prompts (i.e. up to 2 digits per operand). \n",
    "\n",
    "b. using invalid prompts. \n",
    "\n",
    "In both cases document where it does well and where it does not with specific examples.\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "**Question 6:** Add the following code to your GPT's temporal cross entropy loss code: `print(tf.reshape(act_at_correct, (N, T)))` The line of code prints the softmax netAct values produced by the net's output layer from the neuron coding the correct next token. Adapt as necessary to make it print these expected values. Run the code provided below. In some detail, interpret why the netActs are high or low in each case and why that makes sense.\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "**Question 7:** Following up from the previous question, notice how the netActs for the 1st char are small yet nonzero in all cases. Why does this happen and explain why the loss cannot get to exactly 0. \n",
    "\n",
    "**When you are done running Q6 and Q7, comment out the print out in the `loss` method.**\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "**Question 8:** In the cell below, call `generate_sequence` on one of the transformers and plug in a single prompt that yields an incorrect answer based on the above validation set print outs. In your call to `generate_sequence`, set the keyword argument `plot_probs` to `True`. Use the plots of the output layer netActs to explain what is going on when the network predicts the incorrect answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e39c09",
   "metadata": {},
   "source": [
    "**Answer 4:**\n",
    "\n",
    "**Answer 5:**\n",
    "\n",
    "**Answer 6:**\n",
    "\n",
    "**Answer 7:**\n",
    "\n",
    "**Answer 8:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21540606",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca29163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 6 TODO: Replace in the code below:\n",
    "# char2ind_map: with your char-to-int dictionary\n",
    "# gpt_add1: with your trained net\n",
    "\n",
    "prompts = [list('33+1=34.##'), list('33+10=43.#'), list('33+33=66.#')]\n",
    "prompts_int_x, prompts_int_y = make_addition_samples_and_labels(prompts, char2ind_map)\n",
    "prompts_int_x = tf.cast(prompts_int_x, tf.int32)\n",
    "prompts_int_y = tf.cast(prompts_int_y, tf.int32)\n",
    "net_acts = gpt_add1(prompts_int_x)\n",
    "gpt_add1.loss(net_acts, prompts_int_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3c4ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 8.\n",
    "# TODO: modify prompt to one that produces an incorrect output. Change any other variables to suit your conventions\n",
    "prompt = '2+87='\n",
    "answer = gpt_add1.generate_sequence(prompt=prompt,\n",
    "                                length=seq_len,\n",
    "                                char2ind_map=char2ind_map,\n",
    "                                ind2char_map=ind2char_map,\n",
    "                                end_char='.',\n",
    "                                plot_probs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53deffc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs444",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
