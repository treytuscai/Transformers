{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4161dcc7",
   "metadata": {},
   "source": [
    "Trey Tuscai and Gordon Doore\n",
    "\n",
    "Spring 2025\n",
    "\n",
    "CS 444: Deep Learning\n",
    "\n",
    "#### Project 4: Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b90adbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use(['seaborn-v0_8-colorblind', 'seaborn-v0_8-darkgrid'])\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "np.set_printoptions(suppress=True, precision=4)\n",
    "\n",
    "# Automatically reload your external source code\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4e8b16",
   "metadata": {},
   "source": [
    "![Some fun](images/transformer2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6bece8",
   "metadata": {},
   "source": [
    "## Task 5: Create GPT networks\n",
    "\n",
    "With the Transformer Block and other network components implemented, let's build an actual transformer! We are implementing a transformer that follows the design of OpenAI's GPT line of neural networks. Therefore, the network class is called `GPT` (still inherits from `DeepNetwork` like usual). Specific GPT networks that you will construct will be child classes of `GPT` (much like `ResNet8` and `ResNet18` were child classes of `ResNet`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7c7f91",
   "metadata": {},
   "source": [
    "### 5a. Building `GPTPico1`\n",
    "\n",
    "Let's build a minimal GPT that consists of only one Transformer Block. This network, `GPTPico1`, has the following architecture:\n",
    "1. Embedding layer\n",
    "2. Positional encoding block.\n",
    "3. Transformer block (1x)\n",
    "4. Dense output layer\n",
    "\n",
    "Implement and test the following required methods:\n",
    "- `GPT`: constructor.\n",
    "- `GPT`: `__call__` method. Forward pass through the transformer.\n",
    "- `GPTPico1`: constructor. Assemble the net (*see above*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fd0ad70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpts import GPT, GPTPico1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4b7428",
   "metadata": {},
   "source": [
    "#### Test: `loss` (temporal cross-entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cc07f558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When not masking out the padding char, your loss is 0.9513 and it should be 0.9513.\n",
      "When masking out the padding char, your loss is 0.9442 and it should be 0.9442.\n"
     ]
    }
   ],
   "source": [
    "mygpt = GPT(seq_len=4, padding_char_enc=10)\n",
    "mygpt.loss_name = 'temporal_cross_entropy'\n",
    "\n",
    "tf.random.set_seed(1)\n",
    "y_pred = tf.random.uniform(shape=(5, 4, 12), maxval=1, dtype=tf.float32)\n",
    "y_true = tf.random.uniform(shape=(5, 4), maxval=11, dtype=tf.int32)\n",
    "\n",
    "# Test 1: no masking the padding char\n",
    "loss = mygpt.loss(y_pred, y_true, mask_padding_preds=False)\n",
    "print(f'When not masking out the padding char, your loss is {loss.numpy():.4f} and it should be 0.9513.')\n",
    "# Test 2: masking the padding char\n",
    "loss = mygpt.loss(y_pred, y_true)\n",
    "print(f'When masking out the padding char, your loss is {loss.numpy():.4f} and it should be 0.9442.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a658a3f6",
   "metadata": {},
   "source": [
    "#### Test: `GPTPico1` architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ecc6f304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "Dense layer output(output) shape: [1, 15, 9]\n",
      "TransformerBlock_0:\n",
      "\tTransformerBlock_0/MLP:\n",
      "\tDropout layer output(TransformerBlock_0/MLP/dropout) shape: [1, 15, 24]\n",
      "\tDense layer output(TransformerBlock_0/MLP/dense_1) shape: [1, 15, 24]\n",
      "\tDense layer output(TransformerBlock_0/MLP/dense_0) shape: [1, 15, 96]\n",
      "\tTransformerBlock_0/multihead_attention:\n",
      "\tDropout layer output(TransformerBlock_0/multihead_attention/dropout) shape: [1, 15, 24]\n",
      "\tDense layer output(TransformerBlock_0/multihead_attention/dense_1) shape: [1, 15, 24]\n",
      "\tTransformerBlock_0/multihead_attention/attention:\n",
      "\tDropout layer output(TransformerBlock_0/multihead_attention/attention/dropout) shape: [1, 4, 15, 15]\n",
      "\tTransformerBlock_0/multihead_attention/qkv_block:\n",
      "\tDense layer output(TransformerBlock_0/multihead_attention/qkv_block/dense_v) shape: [1, 15, 24]\n",
      "\tDense layer output(TransformerBlock_0/multihead_attention/qkv_block/dense_k) shape: [1, 15, 24]\n",
      "\tDense layer output(TransformerBlock_0/multihead_attention/qkv_block/dense_q) shape: [1, 15, 24]\n",
      "PositionalEncodingBlock:\n",
      "\tDropout layer output(PositionalEncodingBlock/dropout) shape: [1, 15, 24]\n",
      "\tPositional encoding layer output(PositionalEncodingBlock/positional_enc_layer) shape: [1, 15, 24]\n",
      "Embedding layer output(EmbeddingLayer) shape: [1, 15, 24]\n",
      "---------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "mypicogpt = GPTPico1(vocab_sz=9, seq_len=15, padding_char_enc=5)\n",
    "mypicogpt.compile(loss='temporal_cross_entropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a39dc3e",
   "metadata": {},
   "source": [
    "The above cell should print out:\n",
    "\n",
    "```\n",
    "---------------------------------------------------------------------------\n",
    "Dense layer output(output) shape: [1, 15, 9]\n",
    "TransformerBlock_0:\n",
    "\tTransformerBlock_0/MLP:\n",
    "\tDropout layer output(TransformerBlock_0/MLP/dropout) shape: [1, 15, 24]\n",
    "\tDense layer output(TransformerBlock_0/MLP/dense_1) shape: [1, 15, 24]\n",
    "\tDense layer output(TransformerBlock_0/MLP/dense_0) shape: [1, 15, 96]\n",
    "\tTransformerBlock_0/multihead_attention:\n",
    "\tDropout layer output(TransformerBlock_0/multihead_attention/dropout) shape: [1, 15, 24]\n",
    "\tDense layer output(TransformerBlock_0/multihead_attention/dense_1) shape: [1, 15, 24]\n",
    "\tTransformerBlock_0/multihead_attention/attention:\n",
    "\tDropout layer output(TransformerBlock_0/multihead_attention/attention/dropout) shape: [1, 4, 15, 15]\n",
    "\tTransformerBlock_0/multihead_attention/qkv_block:\n",
    "\tDense layer output(TransformerBlock_0/multihead_attention/qkv_block/dense_v) shape: [1, 15, 24]\n",
    "\tDense layer output(TransformerBlock_0/multihead_attention/qkv_block/dense_k) shape: [1, 15, 24]\n",
    "\tDense layer output(TransformerBlock_0/multihead_attention/qkv_block/dense_q) shape: [1, 15, 24]\n",
    "PositionalEncodingBlock:\n",
    "\tDropout layer output(PositionalEncodingBlock/dropout) shape: [1, 15, 24]\n",
    "\tPositional encoding layer output(PositionalEncodingBlock/positional_enc_layer) shape: [1, 15, 24]\n",
    "Embedding layer output(EmbeddingLayer) shape: [1, 15, 24]\n",
    "---------------------------------------------------------------------------\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516e45bc",
   "metadata": {},
   "source": [
    "### 5b. `GPTPico1` overfit test\n",
    "\n",
    "Let's verify that your `GPTPico1` works by overfitting a small amount of fake data.\n",
    "\n",
    "Running the following cell should result in a training loss of ~`0.03` by 500 training epochs. The training loss after 1 epoch should be ~`5.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "04512aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "Dense layer output(output) shape: [1, 5, 100]\n",
      "TransformerBlock_0:\n",
      "\tTransformerBlock_0/MLP:\n",
      "\tDropout layer output(TransformerBlock_0/MLP/dropout) shape: [1, 5, 24]\n",
      "\tDense layer output(TransformerBlock_0/MLP/dense_1) shape: [1, 5, 24]\n",
      "\tDense layer output(TransformerBlock_0/MLP/dense_0) shape: [1, 5, 96]\n",
      "\tTransformerBlock_0/multihead_attention:\n",
      "\tDropout layer output(TransformerBlock_0/multihead_attention/dropout) shape: [1, 5, 24]\n",
      "\tDense layer output(TransformerBlock_0/multihead_attention/dense_1) shape: [1, 5, 24]\n",
      "\tTransformerBlock_0/multihead_attention/attention:\n",
      "\tDropout layer output(TransformerBlock_0/multihead_attention/attention/dropout) shape: [1, 4, 5, 5]\n",
      "\tTransformerBlock_0/multihead_attention/qkv_block:\n",
      "\tDense layer output(TransformerBlock_0/multihead_attention/qkv_block/dense_v) shape: [1, 5, 24]\n",
      "\tDense layer output(TransformerBlock_0/multihead_attention/qkv_block/dense_k) shape: [1, 5, 24]\n",
      "\tDense layer output(TransformerBlock_0/multihead_attention/qkv_block/dense_q) shape: [1, 5, 24]\n",
      "PositionalEncodingBlock:\n",
      "\tDropout layer output(PositionalEncodingBlock/dropout) shape: [1, 5, 24]\n",
      "\tPositional encoding layer output(PositionalEncodingBlock/positional_enc_layer) shape: [1, 5, 24]\n",
      "Embedding layer output(EmbeddingLayer) shape: [1, 5, 24]\n",
      "---------------------------------------------------------------------------\n",
      "Epoch 1: Training Loss = 5.1594, Validation Loss = 5.0166, Validation Accuracy = 0.0000\n",
      "Epoch 1/500 took 3.8766 seconds\n",
      "Epoch 2: Training Loss = 5.1297, Validation Loss = 4.9322, Validation Accuracy = 0.0000\n",
      "Epoch 2/500 took 0.0528 seconds\n",
      "Epoch 3: Training Loss = 4.9596, Validation Loss = 4.8519, Validation Accuracy = 0.0000\n",
      "Epoch 3/500 took 0.0456 seconds\n",
      "Epoch 4: Training Loss = 4.8644, Validation Loss = 4.7765, Validation Accuracy = 0.0000\n",
      "Epoch 4/500 took 0.0472 seconds\n",
      "Epoch 5: Training Loss = 4.8770, Validation Loss = 4.7064, Validation Accuracy = 0.0000\n",
      "Epoch 5/500 took 0.0405 seconds\n",
      "Epoch 6: Training Loss = 4.7466, Validation Loss = 4.6395, Validation Accuracy = 0.0200\n",
      "Epoch 6/500 took 0.0643 seconds\n",
      "Epoch 7: Training Loss = 4.7900, Validation Loss = 4.5766, Validation Accuracy = 0.0200\n",
      "Epoch 7/500 took 0.0710 seconds\n",
      "Epoch 8: Training Loss = 4.6211, Validation Loss = 4.5160, Validation Accuracy = 0.0200\n",
      "Epoch 8/500 took 0.0551 seconds\n",
      "Epoch 9: Training Loss = 4.5663, Validation Loss = 4.4579, Validation Accuracy = 0.0400\n",
      "Epoch 9/500 took 0.0614 seconds\n",
      "Epoch 10: Training Loss = 4.6481, Validation Loss = 4.4046, Validation Accuracy = 0.0600\n",
      "Epoch 10/500 took 0.0709 seconds\n",
      "Epoch 11: Training Loss = 4.4073, Validation Loss = 4.3546, Validation Accuracy = 0.0600\n",
      "Epoch 11/500 took 0.0507 seconds\n",
      "Epoch 12: Training Loss = 4.3925, Validation Loss = 4.3065, Validation Accuracy = 0.0600\n",
      "Epoch 12/500 took 0.0427 seconds\n",
      "Epoch 13: Training Loss = 4.3339, Validation Loss = 4.2615, Validation Accuracy = 0.0600\n",
      "Epoch 13/500 took 0.0430 seconds\n",
      "Epoch 14: Training Loss = 4.1936, Validation Loss = 4.2173, Validation Accuracy = 0.0600\n",
      "Epoch 14/500 took 0.0426 seconds\n",
      "Epoch 15: Training Loss = 4.1153, Validation Loss = 4.1742, Validation Accuracy = 0.0800\n",
      "Epoch 15/500 took 0.0403 seconds\n",
      "Epoch 16: Training Loss = 4.0735, Validation Loss = 4.1334, Validation Accuracy = 0.0600\n",
      "Epoch 16/500 took 0.0376 seconds\n",
      "Epoch 17: Training Loss = 4.2706, Validation Loss = 4.0940, Validation Accuracy = 0.1000\n",
      "Epoch 17/500 took 0.0357 seconds\n",
      "Epoch 18: Training Loss = 4.1583, Validation Loss = 4.0567, Validation Accuracy = 0.1000\n",
      "Epoch 18/500 took 0.0370 seconds\n",
      "Epoch 19: Training Loss = 3.9073, Validation Loss = 4.0195, Validation Accuracy = 0.1200\n",
      "Epoch 19/500 took 0.0452 seconds\n",
      "Epoch 20: Training Loss = 4.0394, Validation Loss = 3.9837, Validation Accuracy = 0.1600\n",
      "Epoch 20/500 took 0.0546 seconds\n",
      "Epoch 21: Training Loss = 3.9985, Validation Loss = 3.9488, Validation Accuracy = 0.1600\n",
      "Epoch 21/500 took 0.0579 seconds\n",
      "Epoch 22: Training Loss = 3.9254, Validation Loss = 3.9136, Validation Accuracy = 0.1600\n",
      "Epoch 22/500 took 0.0330 seconds\n",
      "Epoch 23: Training Loss = 3.8661, Validation Loss = 3.8791, Validation Accuracy = 0.1600\n",
      "Epoch 23/500 took 0.0289 seconds\n",
      "Epoch 24: Training Loss = 3.8826, Validation Loss = 3.8444, Validation Accuracy = 0.1600\n",
      "Epoch 24/500 took 0.0295 seconds\n",
      "Epoch 25: Training Loss = 3.7877, Validation Loss = 3.8101, Validation Accuracy = 0.1400\n",
      "Epoch 25/500 took 0.0284 seconds\n",
      "Epoch 26: Training Loss = 3.7146, Validation Loss = 3.7752, Validation Accuracy = 0.1600\n",
      "Epoch 26/500 took 0.0287 seconds\n",
      "Epoch 27: Training Loss = 3.9794, Validation Loss = 3.7385, Validation Accuracy = 0.1600\n",
      "Epoch 27/500 took 0.0293 seconds\n",
      "Epoch 28: Training Loss = 3.6924, Validation Loss = 3.7004, Validation Accuracy = 0.1600\n",
      "Epoch 28/500 took 0.0304 seconds\n",
      "Epoch 29: Training Loss = 3.7350, Validation Loss = 3.6620, Validation Accuracy = 0.1600\n",
      "Epoch 29/500 took 0.0277 seconds\n",
      "Epoch 30: Training Loss = 3.6413, Validation Loss = 3.6224, Validation Accuracy = 0.1800\n",
      "Epoch 30/500 took 0.0284 seconds\n",
      "Epoch 31: Training Loss = 3.7757, Validation Loss = 3.5809, Validation Accuracy = 0.1800\n",
      "Epoch 31/500 took 0.0283 seconds\n",
      "Epoch 32: Training Loss = 3.8218, Validation Loss = 3.5377, Validation Accuracy = 0.1800\n",
      "Epoch 32/500 took 0.0294 seconds\n",
      "Epoch 33: Training Loss = 3.7027, Validation Loss = 3.4929, Validation Accuracy = 0.1800\n",
      "Epoch 33/500 took 0.0282 seconds\n",
      "Epoch 34: Training Loss = 3.5022, Validation Loss = 3.4467, Validation Accuracy = 0.1800\n",
      "Epoch 34/500 took 0.0300 seconds\n",
      "Epoch 35: Training Loss = 3.3026, Validation Loss = 3.4014, Validation Accuracy = 0.2000\n",
      "Epoch 35/500 took 0.0286 seconds\n",
      "Epoch 36: Training Loss = 3.4273, Validation Loss = 3.3558, Validation Accuracy = 0.2600\n",
      "Epoch 36/500 took 0.0279 seconds\n",
      "Epoch 37: Training Loss = 3.3203, Validation Loss = 3.3093, Validation Accuracy = 0.3200\n",
      "Epoch 37/500 took 0.0277 seconds\n",
      "Epoch 38: Training Loss = 3.4088, Validation Loss = 3.2637, Validation Accuracy = 0.3200\n",
      "Epoch 38/500 took 0.0282 seconds\n",
      "Epoch 39: Training Loss = 2.9548, Validation Loss = 3.2193, Validation Accuracy = 0.3200\n",
      "Epoch 39/500 took 0.0287 seconds\n",
      "Epoch 40: Training Loss = 3.1255, Validation Loss = 3.1738, Validation Accuracy = 0.3400\n",
      "Epoch 40/500 took 0.0275 seconds\n",
      "Epoch 41: Training Loss = 3.0900, Validation Loss = 3.1287, Validation Accuracy = 0.3600\n",
      "Epoch 41/500 took 0.0297 seconds\n",
      "Epoch 42: Training Loss = 3.2876, Validation Loss = 3.0849, Validation Accuracy = 0.4200\n",
      "Epoch 42/500 took 0.0300 seconds\n",
      "Epoch 43: Training Loss = 3.0601, Validation Loss = 3.0416, Validation Accuracy = 0.4200\n",
      "Epoch 43/500 took 0.0283 seconds\n",
      "Epoch 44: Training Loss = 3.1883, Validation Loss = 2.9984, Validation Accuracy = 0.4600\n",
      "Epoch 44/500 took 0.0293 seconds\n",
      "Epoch 45: Training Loss = 3.0550, Validation Loss = 2.9571, Validation Accuracy = 0.4600\n",
      "Epoch 45/500 took 0.0287 seconds\n",
      "Epoch 46: Training Loss = 2.9336, Validation Loss = 2.9170, Validation Accuracy = 0.4800\n",
      "Epoch 46/500 took 0.0290 seconds\n",
      "Epoch 47: Training Loss = 2.9972, Validation Loss = 2.8784, Validation Accuracy = 0.4800\n",
      "Epoch 47/500 took 0.0286 seconds\n",
      "Epoch 48: Training Loss = 2.9392, Validation Loss = 2.8407, Validation Accuracy = 0.4800\n",
      "Epoch 48/500 took 0.0288 seconds\n",
      "Epoch 49: Training Loss = 2.7110, Validation Loss = 2.8004, Validation Accuracy = 0.5200\n",
      "Epoch 49/500 took 0.0306 seconds\n",
      "Epoch 50: Training Loss = 2.6531, Validation Loss = 2.7602, Validation Accuracy = 0.5400\n",
      "Epoch 50/500 took 0.0302 seconds\n",
      "Epoch 51: Training Loss = 2.6584, Validation Loss = 2.7205, Validation Accuracy = 0.5400\n",
      "Epoch 51/500 took 0.0276 seconds\n",
      "Epoch 52: Training Loss = 2.8630, Validation Loss = 2.6803, Validation Accuracy = 0.5400\n",
      "Epoch 52/500 took 0.0274 seconds\n",
      "Epoch 53: Training Loss = 2.6912, Validation Loss = 2.6406, Validation Accuracy = 0.5600\n",
      "Epoch 53/500 took 0.0285 seconds\n",
      "Epoch 54: Training Loss = 2.6886, Validation Loss = 2.6029, Validation Accuracy = 0.5600\n",
      "Epoch 54/500 took 0.0276 seconds\n",
      "Epoch 55: Training Loss = 2.6089, Validation Loss = 2.5668, Validation Accuracy = 0.5600\n",
      "Epoch 55/500 took 0.0274 seconds\n",
      "Epoch 56: Training Loss = 2.6813, Validation Loss = 2.5305, Validation Accuracy = 0.5600\n",
      "Epoch 56/500 took 0.0293 seconds\n",
      "Epoch 57: Training Loss = 2.6287, Validation Loss = 2.4926, Validation Accuracy = 0.5800\n",
      "Epoch 57/500 took 0.0298 seconds\n",
      "Epoch 58: Training Loss = 2.5044, Validation Loss = 2.4555, Validation Accuracy = 0.5800\n",
      "Epoch 58/500 took 0.0273 seconds\n",
      "Epoch 59: Training Loss = 2.5746, Validation Loss = 2.4169, Validation Accuracy = 0.5800\n",
      "Epoch 59/500 took 0.0268 seconds\n",
      "Epoch 60: Training Loss = 2.3919, Validation Loss = 2.3788, Validation Accuracy = 0.6000\n",
      "Epoch 60/500 took 0.0283 seconds\n",
      "Epoch 61: Training Loss = 2.3181, Validation Loss = 2.3409, Validation Accuracy = 0.6000\n",
      "Epoch 61/500 took 0.0273 seconds\n",
      "Epoch 62: Training Loss = 2.3370, Validation Loss = 2.3039, Validation Accuracy = 0.6000\n",
      "Epoch 62/500 took 0.0274 seconds\n",
      "Epoch 63: Training Loss = 2.3329, Validation Loss = 2.2684, Validation Accuracy = 0.6200\n",
      "Epoch 63/500 took 0.0277 seconds\n",
      "Epoch 64: Training Loss = 2.2649, Validation Loss = 2.2338, Validation Accuracy = 0.6400\n",
      "Epoch 64/500 took 0.0332 seconds\n",
      "Epoch 65: Training Loss = 2.2105, Validation Loss = 2.1994, Validation Accuracy = 0.6600\n",
      "Epoch 65/500 took 0.0281 seconds\n",
      "Epoch 66: Training Loss = 2.2235, Validation Loss = 2.1649, Validation Accuracy = 0.7000\n",
      "Epoch 66/500 took 0.0271 seconds\n",
      "Epoch 67: Training Loss = 2.2411, Validation Loss = 2.1290, Validation Accuracy = 0.7000\n",
      "Epoch 67/500 took 0.0281 seconds\n",
      "Epoch 68: Training Loss = 2.1167, Validation Loss = 2.0932, Validation Accuracy = 0.7200\n",
      "Epoch 68/500 took 0.0274 seconds\n",
      "Epoch 69: Training Loss = 2.1315, Validation Loss = 2.0571, Validation Accuracy = 0.7200\n",
      "Epoch 69/500 took 0.0276 seconds\n",
      "Epoch 70: Training Loss = 1.9269, Validation Loss = 2.0213, Validation Accuracy = 0.7200\n",
      "Epoch 70/500 took 0.0273 seconds\n",
      "Epoch 71: Training Loss = 1.9279, Validation Loss = 1.9865, Validation Accuracy = 0.7400\n",
      "Epoch 71/500 took 0.0280 seconds\n",
      "Epoch 72: Training Loss = 1.9910, Validation Loss = 1.9531, Validation Accuracy = 0.7400\n",
      "Epoch 72/500 took 0.0284 seconds\n",
      "Epoch 73: Training Loss = 1.8656, Validation Loss = 1.9200, Validation Accuracy = 0.7400\n",
      "Epoch 73/500 took 0.0288 seconds\n",
      "Epoch 74: Training Loss = 1.8904, Validation Loss = 1.8876, Validation Accuracy = 0.7400\n",
      "Epoch 74/500 took 0.0281 seconds\n",
      "Epoch 75: Training Loss = 1.8162, Validation Loss = 1.8560, Validation Accuracy = 0.7400\n",
      "Epoch 75/500 took 0.0281 seconds\n",
      "Epoch 76: Training Loss = 1.8476, Validation Loss = 1.8252, Validation Accuracy = 0.7400\n",
      "Epoch 76/500 took 0.0272 seconds\n",
      "Epoch 77: Training Loss = 1.9780, Validation Loss = 1.7923, Validation Accuracy = 0.7400\n",
      "Epoch 77/500 took 0.0273 seconds\n",
      "Epoch 78: Training Loss = 1.7180, Validation Loss = 1.7616, Validation Accuracy = 0.7400\n",
      "Epoch 78/500 took 0.0292 seconds\n",
      "Epoch 79: Training Loss = 1.8521, Validation Loss = 1.7313, Validation Accuracy = 0.7600\n",
      "Epoch 79/500 took 0.0277 seconds\n",
      "Epoch 80: Training Loss = 1.7243, Validation Loss = 1.6985, Validation Accuracy = 0.8000\n",
      "Epoch 80/500 took 0.0291 seconds\n",
      "Epoch 81: Training Loss = 1.6766, Validation Loss = 1.6635, Validation Accuracy = 0.8200\n",
      "Epoch 81/500 took 0.0318 seconds\n",
      "Epoch 82: Training Loss = 1.7732, Validation Loss = 1.6301, Validation Accuracy = 0.8200\n",
      "Epoch 82/500 took 0.0314 seconds\n",
      "Epoch 83: Training Loss = 1.6219, Validation Loss = 1.5982, Validation Accuracy = 0.8400\n",
      "Epoch 83/500 took 0.0278 seconds\n",
      "Epoch 84: Training Loss = 1.6638, Validation Loss = 1.5672, Validation Accuracy = 0.8400\n",
      "Epoch 84/500 took 0.0272 seconds\n",
      "Epoch 85: Training Loss = 1.5538, Validation Loss = 1.5366, Validation Accuracy = 0.8800\n",
      "Epoch 85/500 took 0.0281 seconds\n",
      "Epoch 86: Training Loss = 1.4206, Validation Loss = 1.5057, Validation Accuracy = 0.8800\n",
      "Epoch 86/500 took 0.0274 seconds\n",
      "Epoch 87: Training Loss = 1.4478, Validation Loss = 1.4750, Validation Accuracy = 0.8800\n",
      "Epoch 87/500 took 0.0279 seconds\n",
      "Epoch 88: Training Loss = 1.6216, Validation Loss = 1.4452, Validation Accuracy = 0.9000\n",
      "Epoch 88/500 took 0.0301 seconds\n",
      "Epoch 89: Training Loss = 1.5388, Validation Loss = 1.4179, Validation Accuracy = 0.9000\n",
      "Epoch 89/500 took 0.0282 seconds\n",
      "Epoch 90: Training Loss = 1.4156, Validation Loss = 1.3911, Validation Accuracy = 0.9000\n",
      "Epoch 90/500 took 0.0272 seconds\n",
      "Epoch 91: Training Loss = 1.3485, Validation Loss = 1.3624, Validation Accuracy = 0.9200\n",
      "Epoch 91/500 took 0.0287 seconds\n",
      "Epoch 92: Training Loss = 1.4420, Validation Loss = 1.3333, Validation Accuracy = 0.9200\n",
      "Epoch 92/500 took 0.0294 seconds\n",
      "Epoch 93: Training Loss = 1.3017, Validation Loss = 1.3047, Validation Accuracy = 0.9200\n",
      "Epoch 93/500 took 0.0276 seconds\n",
      "Epoch 94: Training Loss = 1.2940, Validation Loss = 1.2775, Validation Accuracy = 0.9200\n",
      "Epoch 94/500 took 0.0276 seconds\n",
      "Epoch 95: Training Loss = 1.3536, Validation Loss = 1.2518, Validation Accuracy = 0.9200\n",
      "Epoch 95/500 took 0.0299 seconds\n",
      "Epoch 96: Training Loss = 1.2638, Validation Loss = 1.2261, Validation Accuracy = 0.9200\n",
      "Epoch 96/500 took 0.0290 seconds\n",
      "Epoch 97: Training Loss = 1.1553, Validation Loss = 1.2010, Validation Accuracy = 0.9200\n",
      "Epoch 97/500 took 0.0275 seconds\n",
      "Epoch 98: Training Loss = 1.1618, Validation Loss = 1.1752, Validation Accuracy = 0.9200\n",
      "Epoch 98/500 took 0.0270 seconds\n",
      "Epoch 99: Training Loss = 1.0983, Validation Loss = 1.1494, Validation Accuracy = 0.9200\n",
      "Epoch 99/500 took 0.0279 seconds\n",
      "Epoch 100: Training Loss = 1.1340, Validation Loss = 1.1239, Validation Accuracy = 0.9200\n",
      "Epoch 100/500 took 0.0274 seconds\n",
      "Epoch 101: Training Loss = 1.2076, Validation Loss = 1.0983, Validation Accuracy = 0.9400\n",
      "Epoch 101/500 took 0.0273 seconds\n",
      "Epoch 102: Training Loss = 1.0812, Validation Loss = 1.0741, Validation Accuracy = 0.9400\n",
      "Epoch 102/500 took 0.0280 seconds\n",
      "Epoch 103: Training Loss = 0.9908, Validation Loss = 1.0519, Validation Accuracy = 0.9400\n",
      "Epoch 103/500 took 0.0297 seconds\n",
      "Epoch 104: Training Loss = 1.0323, Validation Loss = 1.0305, Validation Accuracy = 0.9400\n",
      "Epoch 104/500 took 0.0279 seconds\n",
      "Epoch 105: Training Loss = 0.8807, Validation Loss = 1.0098, Validation Accuracy = 0.9400\n",
      "Epoch 105/500 took 0.0274 seconds\n",
      "Epoch 106: Training Loss = 1.0263, Validation Loss = 0.9881, Validation Accuracy = 0.9400\n",
      "Epoch 106/500 took 0.0276 seconds\n",
      "Epoch 107: Training Loss = 0.9223, Validation Loss = 0.9680, Validation Accuracy = 0.9400\n",
      "Epoch 107/500 took 0.0277 seconds\n",
      "Epoch 108: Training Loss = 1.0526, Validation Loss = 0.9476, Validation Accuracy = 0.9400\n",
      "Epoch 108/500 took 0.0273 seconds\n",
      "Epoch 109: Training Loss = 0.9134, Validation Loss = 0.9282, Validation Accuracy = 0.9400\n",
      "Epoch 109/500 took 0.0270 seconds\n",
      "Epoch 110: Training Loss = 0.9832, Validation Loss = 0.9084, Validation Accuracy = 0.9400\n",
      "Epoch 110/500 took 0.0276 seconds\n",
      "Epoch 111: Training Loss = 0.9106, Validation Loss = 0.8891, Validation Accuracy = 0.9400\n",
      "Epoch 111/500 took 0.0286 seconds\n",
      "Epoch 112: Training Loss = 0.9807, Validation Loss = 0.8696, Validation Accuracy = 0.9400\n",
      "Epoch 112/500 took 0.0284 seconds\n",
      "Epoch 113: Training Loss = 0.8729, Validation Loss = 0.8513, Validation Accuracy = 0.9400\n",
      "Epoch 113/500 took 0.0277 seconds\n",
      "Epoch 114: Training Loss = 0.8884, Validation Loss = 0.8328, Validation Accuracy = 0.9400\n",
      "Epoch 114/500 took 0.0281 seconds\n",
      "Epoch 115: Training Loss = 0.8560, Validation Loss = 0.8144, Validation Accuracy = 0.9600\n",
      "Epoch 115/500 took 0.0276 seconds\n",
      "Epoch 116: Training Loss = 0.8219, Validation Loss = 0.7960, Validation Accuracy = 0.9800\n",
      "Epoch 116/500 took 0.0270 seconds\n",
      "Epoch 117: Training Loss = 0.8690, Validation Loss = 0.7782, Validation Accuracy = 0.9800\n",
      "Epoch 117/500 took 0.0278 seconds\n",
      "Epoch 118: Training Loss = 0.8002, Validation Loss = 0.7612, Validation Accuracy = 0.9800\n",
      "Epoch 118/500 took 0.0307 seconds\n",
      "Epoch 119: Training Loss = 0.7184, Validation Loss = 0.7447, Validation Accuracy = 0.9800\n",
      "Epoch 119/500 took 0.0290 seconds\n",
      "Epoch 120: Training Loss = 0.8028, Validation Loss = 0.7289, Validation Accuracy = 0.9800\n",
      "Epoch 120/500 took 0.0285 seconds\n",
      "Epoch 121: Training Loss = 0.6500, Validation Loss = 0.7135, Validation Accuracy = 0.9800\n",
      "Epoch 121/500 took 0.0278 seconds\n",
      "Epoch 122: Training Loss = 0.7385, Validation Loss = 0.6981, Validation Accuracy = 1.0000\n",
      "Epoch 122/500 took 0.0271 seconds\n",
      "Epoch 123: Training Loss = 0.6887, Validation Loss = 0.6826, Validation Accuracy = 1.0000\n",
      "Epoch 123/500 took 0.0270 seconds\n",
      "Epoch 124: Training Loss = 0.7580, Validation Loss = 0.6677, Validation Accuracy = 1.0000\n",
      "Epoch 124/500 took 0.0279 seconds\n",
      "Epoch 125: Training Loss = 0.7250, Validation Loss = 0.6534, Validation Accuracy = 1.0000\n",
      "Epoch 125/500 took 0.0280 seconds\n",
      "Epoch 126: Training Loss = 0.7444, Validation Loss = 0.6396, Validation Accuracy = 1.0000\n",
      "Epoch 126/500 took 0.0272 seconds\n",
      "Epoch 127: Training Loss = 0.6662, Validation Loss = 0.6259, Validation Accuracy = 1.0000\n",
      "Epoch 127/500 took 0.0289 seconds\n",
      "Epoch 128: Training Loss = 0.5920, Validation Loss = 0.6121, Validation Accuracy = 1.0000\n",
      "Epoch 128/500 took 0.0289 seconds\n",
      "Epoch 129: Training Loss = 0.6071, Validation Loss = 0.5987, Validation Accuracy = 1.0000\n",
      "Epoch 129/500 took 0.0273 seconds\n",
      "Epoch 130: Training Loss = 0.6131, Validation Loss = 0.5858, Validation Accuracy = 1.0000\n",
      "Epoch 130/500 took 0.0270 seconds\n",
      "Epoch 131: Training Loss = 0.6030, Validation Loss = 0.5736, Validation Accuracy = 1.0000\n",
      "Epoch 131/500 took 0.0274 seconds\n",
      "Epoch 132: Training Loss = 0.6038, Validation Loss = 0.5618, Validation Accuracy = 1.0000\n",
      "Epoch 132/500 took 0.0284 seconds\n",
      "Epoch 133: Training Loss = 0.5883, Validation Loss = 0.5504, Validation Accuracy = 1.0000\n",
      "Epoch 133/500 took 0.0275 seconds\n",
      "Epoch 134: Training Loss = 0.5864, Validation Loss = 0.5390, Validation Accuracy = 1.0000\n",
      "Epoch 134/500 took 0.0272 seconds\n",
      "Epoch 135: Training Loss = 0.5366, Validation Loss = 0.5278, Validation Accuracy = 1.0000\n",
      "Epoch 135/500 took 0.0286 seconds\n",
      "Epoch 136: Training Loss = 0.5449, Validation Loss = 0.5166, Validation Accuracy = 1.0000\n",
      "Epoch 136/500 took 0.0281 seconds\n",
      "Epoch 137: Training Loss = 0.4899, Validation Loss = 0.5055, Validation Accuracy = 1.0000\n",
      "Epoch 137/500 took 0.0278 seconds\n",
      "Epoch 138: Training Loss = 0.4696, Validation Loss = 0.4946, Validation Accuracy = 1.0000\n",
      "Epoch 138/500 took 0.0271 seconds\n",
      "Epoch 139: Training Loss = 0.5113, Validation Loss = 0.4835, Validation Accuracy = 1.0000\n",
      "Epoch 139/500 took 0.0282 seconds\n",
      "Epoch 140: Training Loss = 0.5401, Validation Loss = 0.4729, Validation Accuracy = 1.0000\n",
      "Epoch 140/500 took 0.0276 seconds\n",
      "Epoch 141: Training Loss = 0.4638, Validation Loss = 0.4632, Validation Accuracy = 1.0000\n",
      "Epoch 141/500 took 0.0271 seconds\n",
      "Epoch 142: Training Loss = 0.4490, Validation Loss = 0.4540, Validation Accuracy = 1.0000\n",
      "Epoch 142/500 took 0.0275 seconds\n",
      "Epoch 143: Training Loss = 0.4563, Validation Loss = 0.4448, Validation Accuracy = 1.0000\n",
      "Epoch 143/500 took 0.0290 seconds\n",
      "Epoch 144: Training Loss = 0.4579, Validation Loss = 0.4358, Validation Accuracy = 1.0000\n",
      "Epoch 144/500 took 0.0290 seconds\n",
      "Epoch 145: Training Loss = 0.4826, Validation Loss = 0.4269, Validation Accuracy = 1.0000\n",
      "Epoch 145/500 took 0.0272 seconds\n",
      "Epoch 146: Training Loss = 0.4081, Validation Loss = 0.4183, Validation Accuracy = 1.0000\n",
      "Epoch 146/500 took 0.0281 seconds\n",
      "Epoch 147: Training Loss = 0.4603, Validation Loss = 0.4100, Validation Accuracy = 1.0000\n",
      "Epoch 147/500 took 0.0271 seconds\n",
      "Epoch 148: Training Loss = 0.4192, Validation Loss = 0.4018, Validation Accuracy = 1.0000\n",
      "Epoch 148/500 took 0.0271 seconds\n",
      "Epoch 149: Training Loss = 0.3806, Validation Loss = 0.3940, Validation Accuracy = 1.0000\n",
      "Epoch 149/500 took 0.0266 seconds\n",
      "Epoch 150: Training Loss = 0.4401, Validation Loss = 0.3862, Validation Accuracy = 1.0000\n",
      "Epoch 150/500 took 0.0271 seconds\n",
      "Epoch 151: Training Loss = 0.4280, Validation Loss = 0.3788, Validation Accuracy = 1.0000\n",
      "Epoch 151/500 took 0.0291 seconds\n",
      "Epoch 152: Training Loss = 0.3778, Validation Loss = 0.3717, Validation Accuracy = 1.0000\n",
      "Epoch 152/500 took 0.0284 seconds\n",
      "Epoch 153: Training Loss = 0.3764, Validation Loss = 0.3649, Validation Accuracy = 1.0000\n",
      "Epoch 153/500 took 0.0278 seconds\n",
      "Epoch 154: Training Loss = 0.3605, Validation Loss = 0.3584, Validation Accuracy = 1.0000\n",
      "Epoch 154/500 took 0.0277 seconds\n",
      "Epoch 155: Training Loss = 0.3706, Validation Loss = 0.3518, Validation Accuracy = 1.0000\n",
      "Epoch 155/500 took 0.0273 seconds\n",
      "Epoch 156: Training Loss = 0.3707, Validation Loss = 0.3454, Validation Accuracy = 1.0000\n",
      "Epoch 156/500 took 0.0273 seconds\n",
      "Epoch 157: Training Loss = 0.3548, Validation Loss = 0.3389, Validation Accuracy = 1.0000\n",
      "Epoch 157/500 took 0.0274 seconds\n",
      "Epoch 158: Training Loss = 0.3282, Validation Loss = 0.3324, Validation Accuracy = 1.0000\n",
      "Epoch 158/500 took 0.0275 seconds\n",
      "Epoch 159: Training Loss = 0.3125, Validation Loss = 0.3261, Validation Accuracy = 1.0000\n",
      "Epoch 159/500 took 0.0285 seconds\n",
      "Epoch 160: Training Loss = 0.3065, Validation Loss = 0.3201, Validation Accuracy = 1.0000\n",
      "Epoch 160/500 took 0.0288 seconds\n",
      "Epoch 161: Training Loss = 0.3198, Validation Loss = 0.3143, Validation Accuracy = 1.0000\n",
      "Epoch 161/500 took 0.0284 seconds\n",
      "Epoch 162: Training Loss = 0.3173, Validation Loss = 0.3088, Validation Accuracy = 1.0000\n",
      "Epoch 162/500 took 0.0280 seconds\n",
      "Epoch 163: Training Loss = 0.2929, Validation Loss = 0.3036, Validation Accuracy = 1.0000\n",
      "Epoch 163/500 took 0.0274 seconds\n",
      "Epoch 164: Training Loss = 0.3112, Validation Loss = 0.2987, Validation Accuracy = 1.0000\n",
      "Epoch 164/500 took 0.0278 seconds\n",
      "Epoch 165: Training Loss = 0.3286, Validation Loss = 0.2938, Validation Accuracy = 1.0000\n",
      "Epoch 165/500 took 0.0280 seconds\n",
      "Epoch 166: Training Loss = 0.3044, Validation Loss = 0.2893, Validation Accuracy = 1.0000\n",
      "Epoch 166/500 took 0.0273 seconds\n",
      "Epoch 167: Training Loss = 0.2845, Validation Loss = 0.2849, Validation Accuracy = 1.0000\n",
      "Epoch 167/500 took 0.0289 seconds\n",
      "Epoch 168: Training Loss = 0.2785, Validation Loss = 0.2804, Validation Accuracy = 1.0000\n",
      "Epoch 168/500 took 0.0283 seconds\n",
      "Epoch 169: Training Loss = 0.2822, Validation Loss = 0.2759, Validation Accuracy = 1.0000\n",
      "Epoch 169/500 took 0.0278 seconds\n",
      "Epoch 170: Training Loss = 0.2904, Validation Loss = 0.2714, Validation Accuracy = 1.0000\n",
      "Epoch 170/500 took 0.0275 seconds\n",
      "Epoch 171: Training Loss = 0.2729, Validation Loss = 0.2671, Validation Accuracy = 1.0000\n",
      "Epoch 171/500 took 0.0270 seconds\n",
      "Epoch 172: Training Loss = 0.2591, Validation Loss = 0.2628, Validation Accuracy = 1.0000\n",
      "Epoch 172/500 took 0.0281 seconds\n",
      "Epoch 173: Training Loss = 0.2811, Validation Loss = 0.2585, Validation Accuracy = 1.0000\n",
      "Epoch 173/500 took 0.0272 seconds\n",
      "Epoch 174: Training Loss = 0.2435, Validation Loss = 0.2544, Validation Accuracy = 1.0000\n",
      "Epoch 174/500 took 0.0271 seconds\n",
      "Epoch 175: Training Loss = 0.2405, Validation Loss = 0.2504, Validation Accuracy = 1.0000\n",
      "Epoch 175/500 took 0.0289 seconds\n",
      "Epoch 176: Training Loss = 0.2479, Validation Loss = 0.2468, Validation Accuracy = 1.0000\n",
      "Epoch 176/500 took 0.0287 seconds\n",
      "Epoch 177: Training Loss = 0.2449, Validation Loss = 0.2433, Validation Accuracy = 1.0000\n",
      "Epoch 177/500 took 0.0273 seconds\n",
      "Epoch 178: Training Loss = 0.2394, Validation Loss = 0.2397, Validation Accuracy = 1.0000\n",
      "Epoch 178/500 took 0.0274 seconds\n",
      "Epoch 179: Training Loss = 0.2387, Validation Loss = 0.2360, Validation Accuracy = 1.0000\n",
      "Epoch 179/500 took 0.0282 seconds\n",
      "Epoch 180: Training Loss = 0.2587, Validation Loss = 0.2325, Validation Accuracy = 1.0000\n",
      "Epoch 180/500 took 0.0274 seconds\n",
      "Epoch 181: Training Loss = 0.2408, Validation Loss = 0.2292, Validation Accuracy = 1.0000\n",
      "Epoch 181/500 took 0.0273 seconds\n",
      "Epoch 182: Training Loss = 0.2373, Validation Loss = 0.2261, Validation Accuracy = 1.0000\n",
      "Epoch 182/500 took 0.0275 seconds\n",
      "Epoch 183: Training Loss = 0.2316, Validation Loss = 0.2230, Validation Accuracy = 1.0000\n",
      "Epoch 183/500 took 0.0288 seconds\n",
      "Epoch 184: Training Loss = 0.2110, Validation Loss = 0.2199, Validation Accuracy = 1.0000\n",
      "Epoch 184/500 took 0.0292 seconds\n",
      "Epoch 185: Training Loss = 0.2062, Validation Loss = 0.2169, Validation Accuracy = 1.0000\n",
      "Epoch 185/500 took 0.0275 seconds\n",
      "Epoch 186: Training Loss = 0.2365, Validation Loss = 0.2139, Validation Accuracy = 1.0000\n",
      "Epoch 186/500 took 0.0281 seconds\n",
      "Epoch 187: Training Loss = 0.2192, Validation Loss = 0.2111, Validation Accuracy = 1.0000\n",
      "Epoch 187/500 took 0.0275 seconds\n",
      "Epoch 188: Training Loss = 0.2266, Validation Loss = 0.2083, Validation Accuracy = 1.0000\n",
      "Epoch 188/500 took 0.0277 seconds\n",
      "Epoch 189: Training Loss = 0.2155, Validation Loss = 0.2055, Validation Accuracy = 1.0000\n",
      "Epoch 189/500 took 0.0276 seconds\n",
      "Epoch 190: Training Loss = 0.1978, Validation Loss = 0.2028, Validation Accuracy = 1.0000\n",
      "Epoch 190/500 took 0.0285 seconds\n",
      "Epoch 191: Training Loss = 0.2014, Validation Loss = 0.2002, Validation Accuracy = 1.0000\n",
      "Epoch 191/500 took 0.0294 seconds\n",
      "Epoch 192: Training Loss = 0.1955, Validation Loss = 0.1976, Validation Accuracy = 1.0000\n",
      "Epoch 192/500 took 0.0296 seconds\n",
      "Epoch 193: Training Loss = 0.1959, Validation Loss = 0.1951, Validation Accuracy = 1.0000\n",
      "Epoch 193/500 took 0.0285 seconds\n",
      "Epoch 194: Training Loss = 0.1923, Validation Loss = 0.1926, Validation Accuracy = 1.0000\n",
      "Epoch 194/500 took 0.0279 seconds\n",
      "Epoch 195: Training Loss = 0.2055, Validation Loss = 0.1901, Validation Accuracy = 1.0000\n",
      "Epoch 195/500 took 0.0275 seconds\n",
      "Epoch 196: Training Loss = 0.1982, Validation Loss = 0.1877, Validation Accuracy = 1.0000\n",
      "Epoch 196/500 took 0.0275 seconds\n",
      "Epoch 197: Training Loss = 0.1809, Validation Loss = 0.1853, Validation Accuracy = 1.0000\n",
      "Epoch 197/500 took 0.0290 seconds\n",
      "Epoch 198: Training Loss = 0.1957, Validation Loss = 0.1831, Validation Accuracy = 1.0000\n",
      "Epoch 198/500 took 0.0277 seconds\n",
      "Epoch 199: Training Loss = 0.1841, Validation Loss = 0.1809, Validation Accuracy = 1.0000\n",
      "Epoch 199/500 took 0.0294 seconds\n",
      "Epoch 200: Training Loss = 0.1827, Validation Loss = 0.1787, Validation Accuracy = 1.0000\n",
      "Epoch 200/500 took 0.0285 seconds\n",
      "Epoch 201: Training Loss = 0.1783, Validation Loss = 0.1765, Validation Accuracy = 1.0000\n",
      "Epoch 201/500 took 0.0280 seconds\n",
      "Epoch 202: Training Loss = 0.1751, Validation Loss = 0.1744, Validation Accuracy = 1.0000\n",
      "Epoch 202/500 took 0.0275 seconds\n",
      "Epoch 203: Training Loss = 0.1696, Validation Loss = 0.1724, Validation Accuracy = 1.0000\n",
      "Epoch 203/500 took 0.0274 seconds\n",
      "Epoch 204: Training Loss = 0.1663, Validation Loss = 0.1704, Validation Accuracy = 1.0000\n",
      "Epoch 204/500 took 0.0279 seconds\n",
      "Epoch 205: Training Loss = 0.1790, Validation Loss = 0.1684, Validation Accuracy = 1.0000\n",
      "Epoch 205/500 took 0.0280 seconds\n",
      "Epoch 206: Training Loss = 0.1648, Validation Loss = 0.1665, Validation Accuracy = 1.0000\n",
      "Epoch 206/500 took 0.0277 seconds\n",
      "Epoch 207: Training Loss = 0.1714, Validation Loss = 0.1646, Validation Accuracy = 1.0000\n",
      "Epoch 207/500 took 0.0289 seconds\n",
      "Epoch 208: Training Loss = 0.1708, Validation Loss = 0.1627, Validation Accuracy = 1.0000\n",
      "Epoch 208/500 took 0.0289 seconds\n",
      "Epoch 209: Training Loss = 0.1697, Validation Loss = 0.1609, Validation Accuracy = 1.0000\n",
      "Epoch 209/500 took 0.0271 seconds\n",
      "Epoch 210: Training Loss = 0.1502, Validation Loss = 0.1591, Validation Accuracy = 1.0000\n",
      "Epoch 210/500 took 0.0273 seconds\n",
      "Epoch 211: Training Loss = 0.1599, Validation Loss = 0.1573, Validation Accuracy = 1.0000\n",
      "Epoch 211/500 took 0.0285 seconds\n",
      "Epoch 212: Training Loss = 0.1616, Validation Loss = 0.1556, Validation Accuracy = 1.0000\n",
      "Epoch 212/500 took 0.0275 seconds\n",
      "Epoch 213: Training Loss = 0.1577, Validation Loss = 0.1538, Validation Accuracy = 1.0000\n",
      "Epoch 213/500 took 0.0272 seconds\n",
      "Epoch 214: Training Loss = 0.1547, Validation Loss = 0.1522, Validation Accuracy = 1.0000\n",
      "Epoch 214/500 took 0.0272 seconds\n",
      "Epoch 215: Training Loss = 0.1502, Validation Loss = 0.1505, Validation Accuracy = 1.0000\n",
      "Epoch 215/500 took 0.0291 seconds\n",
      "Epoch 216: Training Loss = 0.1521, Validation Loss = 0.1489, Validation Accuracy = 1.0000\n",
      "Epoch 216/500 took 0.0298 seconds\n",
      "Epoch 217: Training Loss = 0.1562, Validation Loss = 0.1473, Validation Accuracy = 1.0000\n",
      "Epoch 217/500 took 0.0273 seconds\n",
      "Epoch 218: Training Loss = 0.1531, Validation Loss = 0.1458, Validation Accuracy = 1.0000\n",
      "Epoch 218/500 took 0.0273 seconds\n",
      "Epoch 219: Training Loss = 0.1423, Validation Loss = 0.1443, Validation Accuracy = 1.0000\n",
      "Epoch 219/500 took 0.0285 seconds\n",
      "Epoch 220: Training Loss = 0.1431, Validation Loss = 0.1428, Validation Accuracy = 1.0000\n",
      "Epoch 220/500 took 0.0276 seconds\n",
      "Epoch 221: Training Loss = 0.1451, Validation Loss = 0.1413, Validation Accuracy = 1.0000\n",
      "Epoch 221/500 took 0.0276 seconds\n",
      "Epoch 222: Training Loss = 0.1371, Validation Loss = 0.1399, Validation Accuracy = 1.0000\n",
      "Epoch 222/500 took 0.0279 seconds\n",
      "Epoch 223: Training Loss = 0.1334, Validation Loss = 0.1385, Validation Accuracy = 1.0000\n",
      "Epoch 223/500 took 0.0293 seconds\n",
      "Epoch 224: Training Loss = 0.1406, Validation Loss = 0.1371, Validation Accuracy = 1.0000\n",
      "Epoch 224/500 took 0.0277 seconds\n",
      "Epoch 225: Training Loss = 0.1450, Validation Loss = 0.1357, Validation Accuracy = 1.0000\n",
      "Epoch 225/500 took 0.0280 seconds\n",
      "Epoch 226: Training Loss = 0.1374, Validation Loss = 0.1344, Validation Accuracy = 1.0000\n",
      "Epoch 226/500 took 0.0282 seconds\n",
      "Epoch 227: Training Loss = 0.1382, Validation Loss = 0.1331, Validation Accuracy = 1.0000\n",
      "Epoch 227/500 took 0.0274 seconds\n",
      "Epoch 228: Training Loss = 0.1385, Validation Loss = 0.1318, Validation Accuracy = 1.0000\n",
      "Epoch 228/500 took 0.0275 seconds\n",
      "Epoch 229: Training Loss = 0.1265, Validation Loss = 0.1305, Validation Accuracy = 1.0000\n",
      "Epoch 229/500 took 0.0275 seconds\n",
      "Epoch 230: Training Loss = 0.1272, Validation Loss = 0.1293, Validation Accuracy = 1.0000\n",
      "Epoch 230/500 took 0.0278 seconds\n",
      "Epoch 231: Training Loss = 0.1302, Validation Loss = 0.1280, Validation Accuracy = 1.0000\n",
      "Epoch 231/500 took 0.0284 seconds\n",
      "Epoch 232: Training Loss = 0.1326, Validation Loss = 0.1268, Validation Accuracy = 1.0000\n",
      "Epoch 232/500 took 0.0287 seconds\n",
      "Epoch 233: Training Loss = 0.1294, Validation Loss = 0.1256, Validation Accuracy = 1.0000\n",
      "Epoch 233/500 took 0.0283 seconds\n",
      "Epoch 234: Training Loss = 0.1290, Validation Loss = 0.1245, Validation Accuracy = 1.0000\n",
      "Epoch 234/500 took 0.0278 seconds\n",
      "Epoch 235: Training Loss = 0.1221, Validation Loss = 0.1233, Validation Accuracy = 1.0000\n",
      "Epoch 235/500 took 0.0272 seconds\n",
      "Epoch 236: Training Loss = 0.1183, Validation Loss = 0.1222, Validation Accuracy = 1.0000\n",
      "Epoch 236/500 took 0.0274 seconds\n",
      "Epoch 237: Training Loss = 0.1193, Validation Loss = 0.1210, Validation Accuracy = 1.0000\n",
      "Epoch 237/500 took 0.0286 seconds\n",
      "Epoch 238: Training Loss = 0.1223, Validation Loss = 0.1199, Validation Accuracy = 1.0000\n",
      "Epoch 238/500 took 0.0267 seconds\n",
      "Epoch 239: Training Loss = 0.1182, Validation Loss = 0.1189, Validation Accuracy = 1.0000\n",
      "Epoch 239/500 took 0.0286 seconds\n",
      "Epoch 240: Training Loss = 0.1236, Validation Loss = 0.1178, Validation Accuracy = 1.0000\n",
      "Epoch 240/500 took 0.0287 seconds\n",
      "Epoch 241: Training Loss = 0.1240, Validation Loss = 0.1167, Validation Accuracy = 1.0000\n",
      "Epoch 241/500 took 0.0274 seconds\n",
      "Epoch 242: Training Loss = 0.1135, Validation Loss = 0.1157, Validation Accuracy = 1.0000\n",
      "Epoch 242/500 took 0.0269 seconds\n",
      "Epoch 243: Training Loss = 0.1150, Validation Loss = 0.1147, Validation Accuracy = 1.0000\n",
      "Epoch 243/500 took 0.0273 seconds\n",
      "Epoch 244: Training Loss = 0.1102, Validation Loss = 0.1137, Validation Accuracy = 1.0000\n",
      "Epoch 244/500 took 0.0281 seconds\n",
      "Epoch 245: Training Loss = 0.1136, Validation Loss = 0.1127, Validation Accuracy = 1.0000\n",
      "Epoch 245/500 took 0.0273 seconds\n",
      "Epoch 246: Training Loss = 0.1085, Validation Loss = 0.1117, Validation Accuracy = 1.0000\n",
      "Epoch 246/500 took 0.0269 seconds\n",
      "Epoch 247: Training Loss = 0.1109, Validation Loss = 0.1108, Validation Accuracy = 1.0000\n",
      "Epoch 247/500 took 0.0293 seconds\n",
      "Epoch 248: Training Loss = 0.1180, Validation Loss = 0.1098, Validation Accuracy = 1.0000\n",
      "Epoch 248/500 took 0.0278 seconds\n",
      "Epoch 249: Training Loss = 0.1120, Validation Loss = 0.1089, Validation Accuracy = 1.0000\n",
      "Epoch 249/500 took 0.0277 seconds\n",
      "Epoch 250: Training Loss = 0.1188, Validation Loss = 0.1080, Validation Accuracy = 1.0000\n",
      "Epoch 250/500 took 0.0275 seconds\n",
      "Epoch 251: Training Loss = 0.1099, Validation Loss = 0.1070, Validation Accuracy = 1.0000\n",
      "Epoch 251/500 took 0.0278 seconds\n",
      "Epoch 252: Training Loss = 0.0976, Validation Loss = 0.1061, Validation Accuracy = 1.0000\n",
      "Epoch 252/500 took 0.0273 seconds\n",
      "Epoch 253: Training Loss = 0.1090, Validation Loss = 0.1053, Validation Accuracy = 1.0000\n",
      "Epoch 253/500 took 0.0271 seconds\n",
      "Epoch 254: Training Loss = 0.1063, Validation Loss = 0.1044, Validation Accuracy = 1.0000\n",
      "Epoch 254/500 took 0.0275 seconds\n",
      "Epoch 255: Training Loss = 0.1030, Validation Loss = 0.1035, Validation Accuracy = 1.0000\n",
      "Epoch 255/500 took 0.0297 seconds\n",
      "Epoch 256: Training Loss = 0.0994, Validation Loss = 0.1027, Validation Accuracy = 1.0000\n",
      "Epoch 256/500 took 0.0289 seconds\n",
      "Epoch 257: Training Loss = 0.1003, Validation Loss = 0.1018, Validation Accuracy = 1.0000\n",
      "Epoch 257/500 took 0.0275 seconds\n",
      "Epoch 258: Training Loss = 0.1024, Validation Loss = 0.1010, Validation Accuracy = 1.0000\n",
      "Epoch 258/500 took 0.0273 seconds\n",
      "Epoch 259: Training Loss = 0.0979, Validation Loss = 0.1002, Validation Accuracy = 1.0000\n",
      "Epoch 259/500 took 0.0272 seconds\n",
      "Epoch 260: Training Loss = 0.1107, Validation Loss = 0.0994, Validation Accuracy = 1.0000\n",
      "Epoch 260/500 took 0.0267 seconds\n",
      "Epoch 261: Training Loss = 0.1029, Validation Loss = 0.0986, Validation Accuracy = 1.0000\n",
      "Epoch 261/500 took 0.0279 seconds\n",
      "Epoch 262: Training Loss = 0.1024, Validation Loss = 0.0978, Validation Accuracy = 1.0000\n",
      "Epoch 262/500 took 0.0273 seconds\n",
      "Epoch 263: Training Loss = 0.0999, Validation Loss = 0.0970, Validation Accuracy = 1.0000\n",
      "Epoch 263/500 took 0.0283 seconds\n",
      "Epoch 264: Training Loss = 0.1018, Validation Loss = 0.0962, Validation Accuracy = 1.0000\n",
      "Epoch 264/500 took 0.0279 seconds\n",
      "Epoch 265: Training Loss = 0.0943, Validation Loss = 0.0954, Validation Accuracy = 1.0000\n",
      "Epoch 265/500 took 0.0280 seconds\n",
      "Epoch 266: Training Loss = 0.0929, Validation Loss = 0.0947, Validation Accuracy = 1.0000\n",
      "Epoch 266/500 took 0.0280 seconds\n",
      "Epoch 267: Training Loss = 0.0960, Validation Loss = 0.0939, Validation Accuracy = 1.0000\n",
      "Epoch 267/500 took 0.0273 seconds\n",
      "Epoch 268: Training Loss = 0.0883, Validation Loss = 0.0932, Validation Accuracy = 1.0000\n",
      "Epoch 268/500 took 0.0272 seconds\n",
      "Epoch 269: Training Loss = 0.0924, Validation Loss = 0.0925, Validation Accuracy = 1.0000\n",
      "Epoch 269/500 took 0.0276 seconds\n",
      "Epoch 270: Training Loss = 0.0931, Validation Loss = 0.0918, Validation Accuracy = 1.0000\n",
      "Epoch 270/500 took 0.0278 seconds\n",
      "Epoch 271: Training Loss = 0.0930, Validation Loss = 0.0911, Validation Accuracy = 1.0000\n",
      "Epoch 271/500 took 0.0291 seconds\n",
      "Epoch 272: Training Loss = 0.0912, Validation Loss = 0.0904, Validation Accuracy = 1.0000\n",
      "Epoch 272/500 took 0.0280 seconds\n",
      "Epoch 273: Training Loss = 0.0872, Validation Loss = 0.0897, Validation Accuracy = 1.0000\n",
      "Epoch 273/500 took 0.0279 seconds\n",
      "Epoch 274: Training Loss = 0.0864, Validation Loss = 0.0890, Validation Accuracy = 1.0000\n",
      "Epoch 274/500 took 0.0274 seconds\n",
      "Epoch 275: Training Loss = 0.0884, Validation Loss = 0.0884, Validation Accuracy = 1.0000\n",
      "Epoch 275/500 took 0.0268 seconds\n",
      "Epoch 276: Training Loss = 0.0839, Validation Loss = 0.0877, Validation Accuracy = 1.0000\n",
      "Epoch 276/500 took 0.0276 seconds\n",
      "Epoch 277: Training Loss = 0.0895, Validation Loss = 0.0871, Validation Accuracy = 1.0000\n",
      "Epoch 277/500 took 0.0277 seconds\n",
      "Epoch 278: Training Loss = 0.0854, Validation Loss = 0.0865, Validation Accuracy = 1.0000\n",
      "Epoch 278/500 took 0.0274 seconds\n",
      "Epoch 279: Training Loss = 0.0900, Validation Loss = 0.0858, Validation Accuracy = 1.0000\n",
      "Epoch 279/500 took 0.0288 seconds\n",
      "Epoch 280: Training Loss = 0.0834, Validation Loss = 0.0852, Validation Accuracy = 1.0000\n",
      "Epoch 280/500 took 0.0292 seconds\n",
      "Epoch 281: Training Loss = 0.0917, Validation Loss = 0.0846, Validation Accuracy = 1.0000\n",
      "Epoch 281/500 took 0.0277 seconds\n",
      "Epoch 282: Training Loss = 0.0768, Validation Loss = 0.0840, Validation Accuracy = 1.0000\n",
      "Epoch 282/500 took 0.0275 seconds\n",
      "Epoch 283: Training Loss = 0.0844, Validation Loss = 0.0834, Validation Accuracy = 1.0000\n",
      "Epoch 283/500 took 0.0276 seconds\n",
      "Epoch 284: Training Loss = 0.0821, Validation Loss = 0.0828, Validation Accuracy = 1.0000\n",
      "Epoch 284/500 took 0.0279 seconds\n",
      "Epoch 285: Training Loss = 0.0823, Validation Loss = 0.0822, Validation Accuracy = 1.0000\n",
      "Epoch 285/500 took 0.0269 seconds\n",
      "Epoch 286: Training Loss = 0.0807, Validation Loss = 0.0817, Validation Accuracy = 1.0000\n",
      "Epoch 286/500 took 0.0266 seconds\n",
      "Epoch 287: Training Loss = 0.0798, Validation Loss = 0.0811, Validation Accuracy = 1.0000\n",
      "Epoch 287/500 took 0.0278 seconds\n",
      "Epoch 288: Training Loss = 0.0822, Validation Loss = 0.0805, Validation Accuracy = 1.0000\n",
      "Epoch 288/500 took 0.0290 seconds\n",
      "Epoch 289: Training Loss = 0.0855, Validation Loss = 0.0800, Validation Accuracy = 1.0000\n",
      "Epoch 289/500 took 0.0274 seconds\n",
      "Epoch 290: Training Loss = 0.0781, Validation Loss = 0.0794, Validation Accuracy = 1.0000\n",
      "Epoch 290/500 took 0.0272 seconds\n",
      "Epoch 291: Training Loss = 0.0804, Validation Loss = 0.0789, Validation Accuracy = 1.0000\n",
      "Epoch 291/500 took 0.0281 seconds\n",
      "Epoch 292: Training Loss = 0.0810, Validation Loss = 0.0783, Validation Accuracy = 1.0000\n",
      "Epoch 292/500 took 0.0272 seconds\n",
      "Epoch 293: Training Loss = 0.0833, Validation Loss = 0.0778, Validation Accuracy = 1.0000\n",
      "Epoch 293/500 took 0.0270 seconds\n",
      "Epoch 294: Training Loss = 0.0749, Validation Loss = 0.0772, Validation Accuracy = 1.0000\n",
      "Epoch 294/500 took 0.0270 seconds\n",
      "Epoch 295: Training Loss = 0.0765, Validation Loss = 0.0767, Validation Accuracy = 1.0000\n",
      "Epoch 295/500 took 0.0293 seconds\n",
      "Epoch 296: Training Loss = 0.0770, Validation Loss = 0.0762, Validation Accuracy = 1.0000\n",
      "Epoch 296/500 took 0.0277 seconds\n",
      "Epoch 297: Training Loss = 0.0817, Validation Loss = 0.0757, Validation Accuracy = 1.0000\n",
      "Epoch 297/500 took 0.0274 seconds\n",
      "Epoch 298: Training Loss = 0.0770, Validation Loss = 0.0752, Validation Accuracy = 1.0000\n",
      "Epoch 298/500 took 0.0280 seconds\n",
      "Epoch 299: Training Loss = 0.0740, Validation Loss = 0.0747, Validation Accuracy = 1.0000\n",
      "Epoch 299/500 took 0.0270 seconds\n",
      "Epoch 300: Training Loss = 0.0700, Validation Loss = 0.0742, Validation Accuracy = 1.0000\n",
      "Epoch 300/500 took 0.0268 seconds\n",
      "Epoch 301: Training Loss = 0.0717, Validation Loss = 0.0737, Validation Accuracy = 1.0000\n",
      "Epoch 301/500 took 0.0270 seconds\n",
      "Epoch 302: Training Loss = 0.0711, Validation Loss = 0.0732, Validation Accuracy = 1.0000\n",
      "Epoch 302/500 took 0.0272 seconds\n",
      "Epoch 303: Training Loss = 0.0721, Validation Loss = 0.0727, Validation Accuracy = 1.0000\n",
      "Epoch 303/500 took 0.0283 seconds\n",
      "Epoch 304: Training Loss = 0.0748, Validation Loss = 0.0722, Validation Accuracy = 1.0000\n",
      "Epoch 304/500 took 0.0291 seconds\n",
      "Epoch 305: Training Loss = 0.0704, Validation Loss = 0.0718, Validation Accuracy = 1.0000\n",
      "Epoch 305/500 took 0.0276 seconds\n",
      "Epoch 306: Training Loss = 0.0737, Validation Loss = 0.0713, Validation Accuracy = 1.0000\n",
      "Epoch 306/500 took 0.0289 seconds\n",
      "Epoch 307: Training Loss = 0.0717, Validation Loss = 0.0709, Validation Accuracy = 1.0000\n",
      "Epoch 307/500 took 0.0269 seconds\n",
      "Epoch 308: Training Loss = 0.0645, Validation Loss = 0.0704, Validation Accuracy = 1.0000\n",
      "Epoch 308/500 took 0.0267 seconds\n",
      "Epoch 309: Training Loss = 0.0739, Validation Loss = 0.0700, Validation Accuracy = 1.0000\n",
      "Epoch 309/500 took 0.0275 seconds\n",
      "Epoch 310: Training Loss = 0.0721, Validation Loss = 0.0695, Validation Accuracy = 1.0000\n",
      "Epoch 310/500 took 0.0277 seconds\n",
      "Epoch 311: Training Loss = 0.0754, Validation Loss = 0.0691, Validation Accuracy = 1.0000\n",
      "Epoch 311/500 took 0.0289 seconds\n",
      "Epoch 312: Training Loss = 0.0701, Validation Loss = 0.0687, Validation Accuracy = 1.0000\n",
      "Epoch 312/500 took 0.0299 seconds\n",
      "Epoch 313: Training Loss = 0.0700, Validation Loss = 0.0682, Validation Accuracy = 1.0000\n",
      "Epoch 313/500 took 0.0282 seconds\n",
      "Epoch 314: Training Loss = 0.0722, Validation Loss = 0.0678, Validation Accuracy = 1.0000\n",
      "Epoch 314/500 took 0.0272 seconds\n",
      "Epoch 315: Training Loss = 0.0662, Validation Loss = 0.0674, Validation Accuracy = 1.0000\n",
      "Epoch 315/500 took 0.0276 seconds\n",
      "Epoch 316: Training Loss = 0.0691, Validation Loss = 0.0670, Validation Accuracy = 1.0000\n",
      "Epoch 316/500 took 0.0275 seconds\n",
      "Epoch 317: Training Loss = 0.0668, Validation Loss = 0.0665, Validation Accuracy = 1.0000\n",
      "Epoch 317/500 took 0.0274 seconds\n",
      "Epoch 318: Training Loss = 0.0666, Validation Loss = 0.0661, Validation Accuracy = 1.0000\n",
      "Epoch 318/500 took 0.0272 seconds\n",
      "Epoch 319: Training Loss = 0.0663, Validation Loss = 0.0657, Validation Accuracy = 1.0000\n",
      "Epoch 319/500 took 0.0593 seconds\n",
      "Epoch 320: Training Loss = 0.0645, Validation Loss = 0.0653, Validation Accuracy = 1.0000\n",
      "Epoch 320/500 took 0.0283 seconds\n",
      "Epoch 321: Training Loss = 0.0652, Validation Loss = 0.0649, Validation Accuracy = 1.0000\n",
      "Epoch 321/500 took 0.0271 seconds\n",
      "Epoch 322: Training Loss = 0.0669, Validation Loss = 0.0645, Validation Accuracy = 1.0000\n",
      "Epoch 322/500 took 0.0271 seconds\n",
      "Epoch 323: Training Loss = 0.0663, Validation Loss = 0.0641, Validation Accuracy = 1.0000\n",
      "Epoch 323/500 took 0.0279 seconds\n",
      "Epoch 324: Training Loss = 0.0597, Validation Loss = 0.0637, Validation Accuracy = 1.0000\n",
      "Epoch 324/500 took 0.0271 seconds\n",
      "Epoch 325: Training Loss = 0.0613, Validation Loss = 0.0633, Validation Accuracy = 1.0000\n",
      "Epoch 325/500 took 0.0273 seconds\n",
      "Epoch 326: Training Loss = 0.0569, Validation Loss = 0.0630, Validation Accuracy = 1.0000\n",
      "Epoch 326/500 took 0.0278 seconds\n",
      "Epoch 327: Training Loss = 0.0639, Validation Loss = 0.0626, Validation Accuracy = 1.0000\n",
      "Epoch 327/500 took 0.0288 seconds\n",
      "Epoch 328: Training Loss = 0.0637, Validation Loss = 0.0622, Validation Accuracy = 1.0000\n",
      "Epoch 328/500 took 0.0283 seconds\n",
      "Epoch 329: Training Loss = 0.0643, Validation Loss = 0.0618, Validation Accuracy = 1.0000\n",
      "Epoch 329/500 took 0.0278 seconds\n",
      "Epoch 330: Training Loss = 0.0595, Validation Loss = 0.0615, Validation Accuracy = 1.0000\n",
      "Epoch 330/500 took 0.0275 seconds\n",
      "Epoch 331: Training Loss = 0.0601, Validation Loss = 0.0611, Validation Accuracy = 1.0000\n",
      "Epoch 331/500 took 0.0270 seconds\n",
      "Epoch 332: Training Loss = 0.0593, Validation Loss = 0.0608, Validation Accuracy = 1.0000\n",
      "Epoch 332/500 took 0.0270 seconds\n",
      "Epoch 333: Training Loss = 0.0604, Validation Loss = 0.0604, Validation Accuracy = 1.0000\n",
      "Epoch 333/500 took 0.0273 seconds\n",
      "Epoch 334: Training Loss = 0.0603, Validation Loss = 0.0601, Validation Accuracy = 1.0000\n",
      "Epoch 334/500 took 0.0276 seconds\n",
      "Epoch 335: Training Loss = 0.0583, Validation Loss = 0.0597, Validation Accuracy = 1.0000\n",
      "Epoch 335/500 took 0.0285 seconds\n",
      "Epoch 336: Training Loss = 0.0624, Validation Loss = 0.0594, Validation Accuracy = 1.0000\n",
      "Epoch 336/500 took 0.0293 seconds\n",
      "Epoch 337: Training Loss = 0.0582, Validation Loss = 0.0591, Validation Accuracy = 1.0000\n",
      "Epoch 337/500 took 0.0282 seconds\n",
      "Epoch 338: Training Loss = 0.0604, Validation Loss = 0.0587, Validation Accuracy = 1.0000\n",
      "Epoch 338/500 took 0.0274 seconds\n",
      "Epoch 339: Training Loss = 0.0571, Validation Loss = 0.0584, Validation Accuracy = 1.0000\n",
      "Epoch 339/500 took 0.0271 seconds\n",
      "Epoch 340: Training Loss = 0.0556, Validation Loss = 0.0581, Validation Accuracy = 1.0000\n",
      "Epoch 340/500 took 0.0269 seconds\n",
      "Epoch 341: Training Loss = 0.0553, Validation Loss = 0.0577, Validation Accuracy = 1.0000\n",
      "Epoch 341/500 took 0.0278 seconds\n",
      "Epoch 342: Training Loss = 0.0547, Validation Loss = 0.0574, Validation Accuracy = 1.0000\n",
      "Epoch 342/500 took 0.0272 seconds\n",
      "Epoch 343: Training Loss = 0.0567, Validation Loss = 0.0571, Validation Accuracy = 1.0000\n",
      "Epoch 343/500 took 0.0287 seconds\n",
      "Epoch 344: Training Loss = 0.0545, Validation Loss = 0.0568, Validation Accuracy = 1.0000\n",
      "Epoch 344/500 took 0.0283 seconds\n",
      "Epoch 345: Training Loss = 0.0537, Validation Loss = 0.0565, Validation Accuracy = 1.0000\n",
      "Epoch 345/500 took 0.0279 seconds\n",
      "Epoch 346: Training Loss = 0.0591, Validation Loss = 0.0562, Validation Accuracy = 1.0000\n",
      "Epoch 346/500 took 0.0268 seconds\n",
      "Epoch 347: Training Loss = 0.0548, Validation Loss = 0.0559, Validation Accuracy = 1.0000\n",
      "Epoch 347/500 took 0.0273 seconds\n",
      "Epoch 348: Training Loss = 0.0552, Validation Loss = 0.0556, Validation Accuracy = 1.0000\n",
      "Epoch 348/500 took 0.0275 seconds\n",
      "Epoch 349: Training Loss = 0.0534, Validation Loss = 0.0553, Validation Accuracy = 1.0000\n",
      "Epoch 349/500 took 0.0272 seconds\n",
      "Epoch 350: Training Loss = 0.0533, Validation Loss = 0.0550, Validation Accuracy = 1.0000\n",
      "Epoch 350/500 took 0.0267 seconds\n",
      "Epoch 351: Training Loss = 0.0548, Validation Loss = 0.0547, Validation Accuracy = 1.0000\n",
      "Epoch 351/500 took 0.0286 seconds\n",
      "Epoch 352: Training Loss = 0.0595, Validation Loss = 0.0544, Validation Accuracy = 1.0000\n",
      "Epoch 352/500 took 0.0276 seconds\n",
      "Epoch 353: Training Loss = 0.0523, Validation Loss = 0.0541, Validation Accuracy = 1.0000\n",
      "Epoch 353/500 took 0.0274 seconds\n",
      "Epoch 354: Training Loss = 0.0543, Validation Loss = 0.0538, Validation Accuracy = 1.0000\n",
      "Epoch 354/500 took 0.0270 seconds\n",
      "Epoch 355: Training Loss = 0.0519, Validation Loss = 0.0535, Validation Accuracy = 1.0000\n",
      "Epoch 355/500 took 0.0274 seconds\n",
      "Epoch 356: Training Loss = 0.0590, Validation Loss = 0.0532, Validation Accuracy = 1.0000\n",
      "Epoch 356/500 took 0.0282 seconds\n",
      "Epoch 357: Training Loss = 0.0543, Validation Loss = 0.0529, Validation Accuracy = 1.0000\n",
      "Epoch 357/500 took 0.0272 seconds\n",
      "Epoch 358: Training Loss = 0.0539, Validation Loss = 0.0527, Validation Accuracy = 1.0000\n",
      "Epoch 358/500 took 0.0270 seconds\n",
      "Epoch 359: Training Loss = 0.0553, Validation Loss = 0.0524, Validation Accuracy = 1.0000\n",
      "Epoch 359/500 took 0.0284 seconds\n",
      "Epoch 360: Training Loss = 0.0523, Validation Loss = 0.0521, Validation Accuracy = 1.0000\n",
      "Epoch 360/500 took 0.0296 seconds\n",
      "Epoch 361: Training Loss = 0.0530, Validation Loss = 0.0518, Validation Accuracy = 1.0000\n",
      "Epoch 361/500 took 0.0278 seconds\n",
      "Epoch 362: Training Loss = 0.0508, Validation Loss = 0.0515, Validation Accuracy = 1.0000\n",
      "Epoch 362/500 took 0.0273 seconds\n",
      "Epoch 363: Training Loss = 0.0541, Validation Loss = 0.0513, Validation Accuracy = 1.0000\n",
      "Epoch 363/500 took 0.0276 seconds\n",
      "Epoch 364: Training Loss = 0.0492, Validation Loss = 0.0510, Validation Accuracy = 1.0000\n",
      "Epoch 364/500 took 0.0277 seconds\n",
      "Epoch 365: Training Loss = 0.0540, Validation Loss = 0.0507, Validation Accuracy = 1.0000\n",
      "Epoch 365/500 took 0.0274 seconds\n",
      "Epoch 366: Training Loss = 0.0503, Validation Loss = 0.0505, Validation Accuracy = 1.0000\n",
      "Epoch 366/500 took 0.0275 seconds\n",
      "Epoch 367: Training Loss = 0.0493, Validation Loss = 0.0502, Validation Accuracy = 1.0000\n",
      "Epoch 367/500 took 0.0290 seconds\n",
      "Epoch 368: Training Loss = 0.0472, Validation Loss = 0.0499, Validation Accuracy = 1.0000\n",
      "Epoch 368/500 took 0.0291 seconds\n",
      "Epoch 369: Training Loss = 0.0532, Validation Loss = 0.0497, Validation Accuracy = 1.0000\n",
      "Epoch 369/500 took 0.0273 seconds\n",
      "Epoch 370: Training Loss = 0.0468, Validation Loss = 0.0494, Validation Accuracy = 1.0000\n",
      "Epoch 370/500 took 0.0276 seconds\n",
      "Epoch 371: Training Loss = 0.0467, Validation Loss = 0.0492, Validation Accuracy = 1.0000\n",
      "Epoch 371/500 took 0.0276 seconds\n",
      "Epoch 372: Training Loss = 0.0523, Validation Loss = 0.0489, Validation Accuracy = 1.0000\n",
      "Epoch 372/500 took 0.0275 seconds\n",
      "Epoch 373: Training Loss = 0.0509, Validation Loss = 0.0487, Validation Accuracy = 1.0000\n",
      "Epoch 373/500 took 0.0275 seconds\n",
      "Epoch 374: Training Loss = 0.0503, Validation Loss = 0.0484, Validation Accuracy = 1.0000\n",
      "Epoch 374/500 took 0.0283 seconds\n",
      "Epoch 375: Training Loss = 0.0480, Validation Loss = 0.0482, Validation Accuracy = 1.0000\n",
      "Epoch 375/500 took 0.0283 seconds\n",
      "Epoch 376: Training Loss = 0.0471, Validation Loss = 0.0479, Validation Accuracy = 1.0000\n",
      "Epoch 376/500 took 0.0280 seconds\n",
      "Epoch 377: Training Loss = 0.0493, Validation Loss = 0.0477, Validation Accuracy = 1.0000\n",
      "Epoch 377/500 took 0.0288 seconds\n",
      "Epoch 378: Training Loss = 0.0475, Validation Loss = 0.0474, Validation Accuracy = 1.0000\n",
      "Epoch 378/500 took 0.0274 seconds\n",
      "Epoch 379: Training Loss = 0.0457, Validation Loss = 0.0472, Validation Accuracy = 1.0000\n",
      "Epoch 379/500 took 0.0273 seconds\n",
      "Epoch 380: Training Loss = 0.0495, Validation Loss = 0.0470, Validation Accuracy = 1.0000\n",
      "Epoch 380/500 took 0.0274 seconds\n",
      "Epoch 381: Training Loss = 0.0519, Validation Loss = 0.0467, Validation Accuracy = 1.0000\n",
      "Epoch 381/500 took 0.0278 seconds\n",
      "Epoch 382: Training Loss = 0.0437, Validation Loss = 0.0465, Validation Accuracy = 1.0000\n",
      "Epoch 382/500 took 0.0268 seconds\n",
      "Epoch 383: Training Loss = 0.0491, Validation Loss = 0.0463, Validation Accuracy = 1.0000\n",
      "Epoch 383/500 took 0.0280 seconds\n",
      "Epoch 384: Training Loss = 0.0439, Validation Loss = 0.0460, Validation Accuracy = 1.0000\n",
      "Epoch 384/500 took 0.0286 seconds\n",
      "Epoch 385: Training Loss = 0.0462, Validation Loss = 0.0458, Validation Accuracy = 1.0000\n",
      "Epoch 385/500 took 0.0276 seconds\n",
      "Epoch 386: Training Loss = 0.0476, Validation Loss = 0.0456, Validation Accuracy = 1.0000\n",
      "Epoch 386/500 took 0.0275 seconds\n",
      "Epoch 387: Training Loss = 0.0430, Validation Loss = 0.0453, Validation Accuracy = 1.0000\n",
      "Epoch 387/500 took 0.0272 seconds\n",
      "Epoch 388: Training Loss = 0.0445, Validation Loss = 0.0451, Validation Accuracy = 1.0000\n",
      "Epoch 388/500 took 0.0274 seconds\n",
      "Epoch 389: Training Loss = 0.0412, Validation Loss = 0.0449, Validation Accuracy = 1.0000\n",
      "Epoch 389/500 took 0.0274 seconds\n",
      "Epoch 390: Training Loss = 0.0453, Validation Loss = 0.0447, Validation Accuracy = 1.0000\n",
      "Epoch 390/500 took 0.0270 seconds\n",
      "Epoch 391: Training Loss = 0.0438, Validation Loss = 0.0445, Validation Accuracy = 1.0000\n",
      "Epoch 391/500 took 0.0285 seconds\n",
      "Epoch 392: Training Loss = 0.0465, Validation Loss = 0.0443, Validation Accuracy = 1.0000\n",
      "Epoch 392/500 took 0.0295 seconds\n",
      "Epoch 393: Training Loss = 0.0464, Validation Loss = 0.0440, Validation Accuracy = 1.0000\n",
      "Epoch 393/500 took 0.0275 seconds\n",
      "Epoch 394: Training Loss = 0.0432, Validation Loss = 0.0438, Validation Accuracy = 1.0000\n",
      "Epoch 394/500 took 0.0271 seconds\n",
      "Epoch 395: Training Loss = 0.0460, Validation Loss = 0.0436, Validation Accuracy = 1.0000\n",
      "Epoch 395/500 took 0.0280 seconds\n",
      "Epoch 396: Training Loss = 0.0433, Validation Loss = 0.0434, Validation Accuracy = 1.0000\n",
      "Epoch 396/500 took 0.0276 seconds\n",
      "Epoch 397: Training Loss = 0.0428, Validation Loss = 0.0432, Validation Accuracy = 1.0000\n",
      "Epoch 397/500 took 0.0272 seconds\n",
      "Epoch 398: Training Loss = 0.0417, Validation Loss = 0.0430, Validation Accuracy = 1.0000\n",
      "Epoch 398/500 took 0.0269 seconds\n",
      "Epoch 399: Training Loss = 0.0427, Validation Loss = 0.0428, Validation Accuracy = 1.0000\n",
      "Epoch 399/500 took 0.0297 seconds\n",
      "Epoch 400: Training Loss = 0.0450, Validation Loss = 0.0426, Validation Accuracy = 1.0000\n",
      "Epoch 400/500 took 0.0274 seconds\n",
      "Epoch 401: Training Loss = 0.0431, Validation Loss = 0.0424, Validation Accuracy = 1.0000\n",
      "Epoch 401/500 took 0.0270 seconds\n",
      "Epoch 402: Training Loss = 0.0411, Validation Loss = 0.0422, Validation Accuracy = 1.0000\n",
      "Epoch 402/500 took 0.0271 seconds\n",
      "Epoch 403: Training Loss = 0.0411, Validation Loss = 0.0420, Validation Accuracy = 1.0000\n",
      "Epoch 403/500 took 0.0273 seconds\n",
      "Epoch 404: Training Loss = 0.0409, Validation Loss = 0.0418, Validation Accuracy = 1.0000\n",
      "Epoch 404/500 took 0.0269 seconds\n",
      "Epoch 405: Training Loss = 0.0412, Validation Loss = 0.0416, Validation Accuracy = 1.0000\n",
      "Epoch 405/500 took 0.0272 seconds\n",
      "Epoch 406: Training Loss = 0.0427, Validation Loss = 0.0414, Validation Accuracy = 1.0000\n",
      "Epoch 406/500 took 0.0281 seconds\n",
      "Epoch 407: Training Loss = 0.0400, Validation Loss = 0.0412, Validation Accuracy = 1.0000\n",
      "Epoch 407/500 took 0.0283 seconds\n",
      "Epoch 408: Training Loss = 0.0422, Validation Loss = 0.0410, Validation Accuracy = 1.0000\n",
      "Epoch 408/500 took 0.0287 seconds\n",
      "Epoch 409: Training Loss = 0.0412, Validation Loss = 0.0408, Validation Accuracy = 1.0000\n",
      "Epoch 409/500 took 0.0272 seconds\n",
      "Epoch 410: Training Loss = 0.0400, Validation Loss = 0.0407, Validation Accuracy = 1.0000\n",
      "Epoch 410/500 took 0.0275 seconds\n",
      "Epoch 411: Training Loss = 0.0406, Validation Loss = 0.0405, Validation Accuracy = 1.0000\n",
      "Epoch 411/500 took 0.0281 seconds\n",
      "Epoch 412: Training Loss = 0.0412, Validation Loss = 0.0403, Validation Accuracy = 1.0000\n",
      "Epoch 412/500 took 0.0272 seconds\n",
      "Epoch 413: Training Loss = 0.0384, Validation Loss = 0.0401, Validation Accuracy = 1.0000\n",
      "Epoch 413/500 took 0.0273 seconds\n",
      "Epoch 414: Training Loss = 0.0395, Validation Loss = 0.0399, Validation Accuracy = 1.0000\n",
      "Epoch 414/500 took 0.0274 seconds\n",
      "Epoch 415: Training Loss = 0.0420, Validation Loss = 0.0397, Validation Accuracy = 1.0000\n",
      "Epoch 415/500 took 0.0286 seconds\n",
      "Epoch 416: Training Loss = 0.0398, Validation Loss = 0.0396, Validation Accuracy = 1.0000\n",
      "Epoch 416/500 took 0.0289 seconds\n",
      "Epoch 417: Training Loss = 0.0406, Validation Loss = 0.0394, Validation Accuracy = 1.0000\n",
      "Epoch 417/500 took 0.0280 seconds\n",
      "Epoch 418: Training Loss = 0.0393, Validation Loss = 0.0392, Validation Accuracy = 1.0000\n",
      "Epoch 418/500 took 0.0278 seconds\n",
      "Epoch 419: Training Loss = 0.0416, Validation Loss = 0.0390, Validation Accuracy = 1.0000\n",
      "Epoch 419/500 took 0.0275 seconds\n",
      "Epoch 420: Training Loss = 0.0374, Validation Loss = 0.0389, Validation Accuracy = 1.0000\n",
      "Epoch 420/500 took 0.0275 seconds\n",
      "Epoch 421: Training Loss = 0.0394, Validation Loss = 0.0387, Validation Accuracy = 1.0000\n",
      "Epoch 421/500 took 0.0275 seconds\n",
      "Epoch 422: Training Loss = 0.0395, Validation Loss = 0.0385, Validation Accuracy = 1.0000\n",
      "Epoch 422/500 took 0.0275 seconds\n",
      "Epoch 423: Training Loss = 0.0364, Validation Loss = 0.0383, Validation Accuracy = 1.0000\n",
      "Epoch 423/500 took 0.0297 seconds\n",
      "Epoch 424: Training Loss = 0.0384, Validation Loss = 0.0382, Validation Accuracy = 1.0000\n",
      "Epoch 424/500 took 0.0281 seconds\n",
      "Epoch 425: Training Loss = 0.0377, Validation Loss = 0.0380, Validation Accuracy = 1.0000\n",
      "Epoch 425/500 took 0.0278 seconds\n",
      "Epoch 426: Training Loss = 0.0426, Validation Loss = 0.0378, Validation Accuracy = 1.0000\n",
      "Epoch 426/500 took 0.0272 seconds\n",
      "Epoch 427: Training Loss = 0.0399, Validation Loss = 0.0377, Validation Accuracy = 1.0000\n",
      "Epoch 427/500 took 0.0272 seconds\n",
      "Epoch 428: Training Loss = 0.0369, Validation Loss = 0.0375, Validation Accuracy = 1.0000\n",
      "Epoch 428/500 took 0.0277 seconds\n",
      "Epoch 429: Training Loss = 0.0404, Validation Loss = 0.0373, Validation Accuracy = 1.0000\n",
      "Epoch 429/500 took 0.0283 seconds\n",
      "Epoch 430: Training Loss = 0.0407, Validation Loss = 0.0372, Validation Accuracy = 1.0000\n",
      "Epoch 430/500 took 0.0272 seconds\n",
      "Epoch 431: Training Loss = 0.0343, Validation Loss = 0.0370, Validation Accuracy = 1.0000\n",
      "Epoch 431/500 took 0.0281 seconds\n",
      "Epoch 432: Training Loss = 0.0386, Validation Loss = 0.0368, Validation Accuracy = 1.0000\n",
      "Epoch 432/500 took 0.0298 seconds\n",
      "Epoch 433: Training Loss = 0.0360, Validation Loss = 0.0367, Validation Accuracy = 1.0000\n",
      "Epoch 433/500 took 0.0273 seconds\n",
      "Epoch 434: Training Loss = 0.0393, Validation Loss = 0.0365, Validation Accuracy = 1.0000\n",
      "Epoch 434/500 took 0.0275 seconds\n",
      "Epoch 435: Training Loss = 0.0355, Validation Loss = 0.0363, Validation Accuracy = 1.0000\n",
      "Epoch 435/500 took 0.0276 seconds\n",
      "Epoch 436: Training Loss = 0.0359, Validation Loss = 0.0362, Validation Accuracy = 1.0000\n",
      "Epoch 436/500 took 0.0276 seconds\n",
      "Epoch 437: Training Loss = 0.0357, Validation Loss = 0.0360, Validation Accuracy = 1.0000\n",
      "Epoch 437/500 took 0.0273 seconds\n",
      "Epoch 438: Training Loss = 0.0364, Validation Loss = 0.0359, Validation Accuracy = 1.0000\n",
      "Epoch 438/500 took 0.0270 seconds\n",
      "Epoch 439: Training Loss = 0.0393, Validation Loss = 0.0357, Validation Accuracy = 1.0000\n",
      "Epoch 439/500 took 0.0333 seconds\n",
      "Epoch 440: Training Loss = 0.0353, Validation Loss = 0.0356, Validation Accuracy = 1.0000\n",
      "Epoch 440/500 took 0.0283 seconds\n",
      "Epoch 441: Training Loss = 0.0350, Validation Loss = 0.0354, Validation Accuracy = 1.0000\n",
      "Epoch 441/500 took 0.0275 seconds\n",
      "Epoch 442: Training Loss = 0.0342, Validation Loss = 0.0353, Validation Accuracy = 1.0000\n",
      "Epoch 442/500 took 0.0279 seconds\n",
      "Epoch 443: Training Loss = 0.0343, Validation Loss = 0.0351, Validation Accuracy = 1.0000\n",
      "Epoch 443/500 took 0.0275 seconds\n",
      "Epoch 444: Training Loss = 0.0311, Validation Loss = 0.0350, Validation Accuracy = 1.0000\n",
      "Epoch 444/500 took 0.0276 seconds\n",
      "Epoch 445: Training Loss = 0.0348, Validation Loss = 0.0348, Validation Accuracy = 1.0000\n",
      "Epoch 445/500 took 0.0276 seconds\n",
      "Epoch 446: Training Loss = 0.0346, Validation Loss = 0.0347, Validation Accuracy = 1.0000\n",
      "Epoch 446/500 took 0.0279 seconds\n",
      "Epoch 447: Training Loss = 0.0357, Validation Loss = 0.0345, Validation Accuracy = 1.0000\n",
      "Epoch 447/500 took 0.0289 seconds\n",
      "Epoch 448: Training Loss = 0.0330, Validation Loss = 0.0344, Validation Accuracy = 1.0000\n",
      "Epoch 448/500 took 0.0287 seconds\n",
      "Epoch 449: Training Loss = 0.0351, Validation Loss = 0.0342, Validation Accuracy = 1.0000\n",
      "Epoch 449/500 took 0.0278 seconds\n",
      "Epoch 450: Training Loss = 0.0331, Validation Loss = 0.0341, Validation Accuracy = 1.0000\n",
      "Epoch 450/500 took 0.0278 seconds\n",
      "Epoch 451: Training Loss = 0.0344, Validation Loss = 0.0340, Validation Accuracy = 1.0000\n",
      "Epoch 451/500 took 0.0276 seconds\n",
      "Epoch 452: Training Loss = 0.0352, Validation Loss = 0.0338, Validation Accuracy = 1.0000\n",
      "Epoch 452/500 took 0.0274 seconds\n",
      "Epoch 453: Training Loss = 0.0339, Validation Loss = 0.0337, Validation Accuracy = 1.0000\n",
      "Epoch 453/500 took 0.0282 seconds\n",
      "Epoch 454: Training Loss = 0.0330, Validation Loss = 0.0335, Validation Accuracy = 1.0000\n",
      "Epoch 454/500 took 0.0280 seconds\n",
      "Epoch 455: Training Loss = 0.0340, Validation Loss = 0.0334, Validation Accuracy = 1.0000\n",
      "Epoch 455/500 took 0.0289 seconds\n",
      "Epoch 456: Training Loss = 0.0332, Validation Loss = 0.0333, Validation Accuracy = 1.0000\n",
      "Epoch 456/500 took 0.0285 seconds\n",
      "Epoch 457: Training Loss = 0.0330, Validation Loss = 0.0331, Validation Accuracy = 1.0000\n",
      "Epoch 457/500 took 0.0288 seconds\n",
      "Epoch 458: Training Loss = 0.0337, Validation Loss = 0.0330, Validation Accuracy = 1.0000\n",
      "Epoch 458/500 took 0.0279 seconds\n",
      "Epoch 459: Training Loss = 0.0341, Validation Loss = 0.0329, Validation Accuracy = 1.0000\n",
      "Epoch 459/500 took 0.0271 seconds\n",
      "Epoch 460: Training Loss = 0.0353, Validation Loss = 0.0327, Validation Accuracy = 1.0000\n",
      "Epoch 460/500 took 0.0275 seconds\n",
      "Epoch 461: Training Loss = 0.0324, Validation Loss = 0.0326, Validation Accuracy = 1.0000\n",
      "Epoch 461/500 took 0.0280 seconds\n",
      "Epoch 462: Training Loss = 0.0321, Validation Loss = 0.0325, Validation Accuracy = 1.0000\n",
      "Epoch 462/500 took 0.0279 seconds\n",
      "Epoch 463: Training Loss = 0.0307, Validation Loss = 0.0323, Validation Accuracy = 1.0000\n",
      "Epoch 463/500 took 0.0296 seconds\n",
      "Epoch 464: Training Loss = 0.0314, Validation Loss = 0.0322, Validation Accuracy = 1.0000\n",
      "Epoch 464/500 took 0.0306 seconds\n",
      "Epoch 465: Training Loss = 0.0317, Validation Loss = 0.0321, Validation Accuracy = 1.0000\n",
      "Epoch 465/500 took 0.0275 seconds\n",
      "Epoch 466: Training Loss = 0.0288, Validation Loss = 0.0319, Validation Accuracy = 1.0000\n",
      "Epoch 466/500 took 0.0280 seconds\n",
      "Epoch 467: Training Loss = 0.0301, Validation Loss = 0.0318, Validation Accuracy = 1.0000\n",
      "Epoch 467/500 took 0.0277 seconds\n",
      "Epoch 468: Training Loss = 0.0320, Validation Loss = 0.0317, Validation Accuracy = 1.0000\n",
      "Epoch 468/500 took 0.0282 seconds\n",
      "Epoch 469: Training Loss = 0.0326, Validation Loss = 0.0316, Validation Accuracy = 1.0000\n",
      "Epoch 469/500 took 0.0279 seconds\n",
      "Epoch 470: Training Loss = 0.0305, Validation Loss = 0.0314, Validation Accuracy = 1.0000\n",
      "Epoch 470/500 took 0.0274 seconds\n",
      "Epoch 471: Training Loss = 0.0311, Validation Loss = 0.0313, Validation Accuracy = 1.0000\n",
      "Epoch 471/500 took 0.0297 seconds\n",
      "Epoch 472: Training Loss = 0.0324, Validation Loss = 0.0312, Validation Accuracy = 1.0000\n",
      "Epoch 472/500 took 0.0286 seconds\n",
      "Epoch 473: Training Loss = 0.0307, Validation Loss = 0.0311, Validation Accuracy = 1.0000\n",
      "Epoch 473/500 took 0.0279 seconds\n",
      "Epoch 474: Training Loss = 0.0313, Validation Loss = 0.0310, Validation Accuracy = 1.0000\n",
      "Epoch 474/500 took 0.0278 seconds\n",
      "Epoch 475: Training Loss = 0.0305, Validation Loss = 0.0308, Validation Accuracy = 1.0000\n",
      "Epoch 475/500 took 0.0285 seconds\n",
      "Epoch 476: Training Loss = 0.0310, Validation Loss = 0.0307, Validation Accuracy = 1.0000\n",
      "Epoch 476/500 took 0.0283 seconds\n",
      "Epoch 477: Training Loss = 0.0297, Validation Loss = 0.0306, Validation Accuracy = 1.0000\n",
      "Epoch 477/500 took 0.0276 seconds\n",
      "Epoch 478: Training Loss = 0.0305, Validation Loss = 0.0305, Validation Accuracy = 1.0000\n",
      "Epoch 478/500 took 0.0279 seconds\n",
      "Epoch 479: Training Loss = 0.0294, Validation Loss = 0.0304, Validation Accuracy = 1.0000\n",
      "Epoch 479/500 took 0.0293 seconds\n",
      "Epoch 480: Training Loss = 0.0306, Validation Loss = 0.0302, Validation Accuracy = 1.0000\n",
      "Epoch 480/500 took 0.0292 seconds\n",
      "Epoch 481: Training Loss = 0.0291, Validation Loss = 0.0301, Validation Accuracy = 1.0000\n",
      "Epoch 481/500 took 0.0282 seconds\n",
      "Epoch 482: Training Loss = 0.0299, Validation Loss = 0.0300, Validation Accuracy = 1.0000\n",
      "Epoch 482/500 took 0.0280 seconds\n",
      "Epoch 483: Training Loss = 0.0299, Validation Loss = 0.0299, Validation Accuracy = 1.0000\n",
      "Epoch 483/500 took 0.0278 seconds\n",
      "Epoch 484: Training Loss = 0.0282, Validation Loss = 0.0298, Validation Accuracy = 1.0000\n",
      "Epoch 484/500 took 0.0275 seconds\n",
      "Epoch 485: Training Loss = 0.0287, Validation Loss = 0.0297, Validation Accuracy = 1.0000\n",
      "Epoch 485/500 took 0.0276 seconds\n",
      "Epoch 486: Training Loss = 0.0284, Validation Loss = 0.0296, Validation Accuracy = 1.0000\n",
      "Epoch 486/500 took 0.0287 seconds\n",
      "Epoch 487: Training Loss = 0.0298, Validation Loss = 0.0295, Validation Accuracy = 1.0000\n",
      "Epoch 487/500 took 0.0294 seconds\n",
      "Epoch 488: Training Loss = 0.0301, Validation Loss = 0.0293, Validation Accuracy = 1.0000\n",
      "Epoch 488/500 took 0.0283 seconds\n",
      "Epoch 489: Training Loss = 0.0292, Validation Loss = 0.0292, Validation Accuracy = 1.0000\n",
      "Epoch 489/500 took 0.0282 seconds\n",
      "Epoch 490: Training Loss = 0.0284, Validation Loss = 0.0291, Validation Accuracy = 1.0000\n",
      "Epoch 490/500 took 0.0284 seconds\n",
      "Epoch 491: Training Loss = 0.0298, Validation Loss = 0.0290, Validation Accuracy = 1.0000\n",
      "Epoch 491/500 took 0.0280 seconds\n",
      "Epoch 492: Training Loss = 0.0297, Validation Loss = 0.0289, Validation Accuracy = 1.0000\n",
      "Epoch 492/500 took 0.0283 seconds\n",
      "Epoch 493: Training Loss = 0.0297, Validation Loss = 0.0288, Validation Accuracy = 1.0000\n",
      "Epoch 493/500 took 0.0289 seconds\n",
      "Epoch 494: Training Loss = 0.0276, Validation Loss = 0.0287, Validation Accuracy = 1.0000\n",
      "Epoch 494/500 took 0.0278 seconds\n",
      "Epoch 495: Training Loss = 0.0286, Validation Loss = 0.0286, Validation Accuracy = 1.0000\n",
      "Epoch 495/500 took 0.0304 seconds\n",
      "Epoch 496: Training Loss = 0.0284, Validation Loss = 0.0285, Validation Accuracy = 1.0000\n",
      "Epoch 496/500 took 0.0296 seconds\n",
      "Epoch 497: Training Loss = 0.0305, Validation Loss = 0.0284, Validation Accuracy = 1.0000\n",
      "Epoch 497/500 took 0.0283 seconds\n",
      "Epoch 498: Training Loss = 0.0283, Validation Loss = 0.0283, Validation Accuracy = 1.0000\n",
      "Epoch 498/500 took 0.0276 seconds\n",
      "Epoch 499: Training Loss = 0.0286, Validation Loss = 0.0282, Validation Accuracy = 1.0000\n",
      "Epoch 499/500 took 0.0272 seconds\n",
      "Epoch 500: Training Loss = 0.0292, Validation Loss = 0.0281, Validation Accuracy = 1.0000\n",
      "Epoch 500/500 took 0.0288 seconds\n",
      "Finished training after 500 epochs!\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(0)\n",
    "mypicogpt = GPTPico1(vocab_sz=100, seq_len=5, padding_char_enc=99)\n",
    "mypicogpt.compile(loss='temporal_cross_entropy')\n",
    "\n",
    "tf.random.set_seed(1)\n",
    "x_dev_test = tf.random.uniform(shape=(10, 5), maxval=100, dtype=tf.int32)\n",
    "y_dev_test = tf.random.uniform(shape=(10, 5), maxval=100, dtype=tf.int32)\n",
    "train_loss_hist, _, _, _ = mypicogpt.fit(x_dev_test, y_dev_test, x_dev_test, y_dev_test, batch_size=10, max_epochs=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc4f815",
   "metadata": {},
   "source": [
    "## Task 6: Train GPTs on the Addition Dataset\n",
    "\n",
    "In task, you will train small transformers on the Addition dataset, have the transformers generate the answers to addition problems, and analyze the properties of the trained transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004ba28c",
   "metadata": {},
   "source": [
    "### 6a. Train `GPTPico1` on a small amount of Addition Dataset expressions\n",
    "\n",
    "In the cell below, train `GPTPico1` on the first 25 samples from the Addition dataset. Use:\n",
    "- the training set as the validation set\n",
    "- the default random seed.\n",
    "- a patience of `15` (no learning rate decay).\n",
    "- a batch size of `25` (*batch gradient descent*).\n",
    "\n",
    "You should get a final training loss of less than `0.20`.\n",
    "\n",
    "Make a well-labeled plot showing the training loss over the course of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98adeb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from addition_dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fe29fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, _, _, char2ind_map = get_addition_dataset(N=25, val_prop=0.)\n",
    "vocab_sz = len(char2ind_map)\n",
    "seq_len = x_train.shape[1]\n",
    "\n",
    "\n",
    "print('First 5 expressions (encoded)')\n",
    "print(x_train[:5].numpy())\n",
    "print('First 5 target labels (encoded)')\n",
    "print(y_train[:5].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ac975d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d794106",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4d9c331",
   "metadata": {},
   "source": [
    "### 6b. Test `generate_sequence`\n",
    "\n",
    "This is the method that allows you to prompt your transformer and have it generate text that follows your prompt.\n",
    "\n",
    "Test it out in the cell below on your GPTPico1 network **trained** on the first 25 addition expressions (*the one that yielded a final training loss of less than `0.20` above*) by prompting it with the left-hand side of a single one of the 25  training samples.\n",
    "\n",
    "For example, if you prompt `'47+51='` (the 1st sample) it should output `98.` (or `'47+51=98.'` if you have chatbot-style character-by-character live printing turned on as it generates).\n",
    "\n",
    "Another example prompt is `'75+95='` (2nd sample), which should output `170.` (or `'75+95=170.'` if live printing is on).\n",
    "\n",
    "**Note:** The end character is set to `'.'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9144069e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind2char_map = make_ind2char_mapping(char2ind_map)\n",
    "\n",
    "\n",
    "answer = gpt_add.generate_sequence(prompt=prompt,\n",
    "                                   length=seq_len,\n",
    "                                   char2ind_map=char2ind_map,\n",
    "                                   ind2char_map=ind2char_map,\n",
    "                                   end_char='.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43ebf42",
   "metadata": {},
   "source": [
    "### 6c. Verify transformer overfits all 25 addition expressions\n",
    "\n",
    "Once you have the above `generate_sequence` test working, \"script\" your GPT to generate the answers to all 25 addition expressions in the current subset of the addition dataset. When you print out the generated answers, also print out the true answer so that you can visually scan the correctness of the outputs. Your `GPTPico1` should get all or the vast majority of additions correct.\n",
    "\n",
    "**Reminders:**\n",
    "- Make sure you set the end character to the appropriate char.\n",
    "- Use the `convert_int2str` and `split_sum_and_answer` methods that you wrote to prepare the prompts and expected answers for each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d776e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from addition_dataset import convert_int2str, split_sum_and_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cd9751",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33aa0bde",
   "metadata": {},
   "source": [
    "### 6d. Transformer learns to add\n",
    "\n",
    "In the cell below, train `GPTPico1` on an addition dataset with `25000` expressions the default configuration (e.g. 90/10 train/val split). Use default network and training hyperparameters except for:\n",
    "- batch size of `1024`.\n",
    "- patience of `15`.\n",
    "- learning rate patience of `5`.\n",
    "- at most `4` learning rate decays.\n",
    "- at most `1200` training epochs.\n",
    "\n",
    "Do the following after training your net:\n",
    "1. Create a well-labeled plot showing the training and validation loss over epochs.\n",
    "2. Print out of the generated answers for the 1st 50 **training AND validation set** samples separately. Each print out should show the prompt (left-hand side of `=`, including the `=`), the answer generated by the transformer, and the correct answer — all in a neat, easy to read format.\n",
    "\n",
    "*If everything is working as expected, you should be able to achieve a validation loss in the 0.9s*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55efb3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from addition_dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20273a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4706b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ba432a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350ce645",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_show = 50\n",
    "x_splits = [x_train, x_val]\n",
    "split_labels = ['train', 'val']\n",
    "\n",
    "for i in range(2):\n",
    "    x = x_splits[i]\n",
    "    split = split_labels[i]\n",
    "\n",
    "    print(50*'=')\n",
    "    print(split)\n",
    "    print(50*'=')\n",
    "    N = len(x)\n",
    "\n",
    "    ind2char_map = make_ind2char_mapping(char2ind_map)\n",
    "    x_str = convert_int2str(x_int=x.numpy(), ind2char_map=ind2char_map)\n",
    "    prompts, correct_answers = split_sum_and_answer(x_str)\n",
    "\n",
    "\n",
    "    for i in range(N_show):\n",
    "        curr_prompt = prompts[i]\n",
    "        curr_ans = correct_answers[i]\n",
    "        answer = gpt_add1.generate_sequence(prompt=curr_prompt,\n",
    "                                            length=seq_len,\n",
    "                                            char2ind_map=char2ind_map,\n",
    "                                            ind2char_map=ind2char_map,\n",
    "                                            end_char='.')\n",
    "        print('Correct answer is:', curr_ans)\n",
    "        print('---------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5959a8a2",
   "metadata": {},
   "source": [
    "### 6e. Questions\n",
    "\n",
    "**Question 4:** Look over the addition prompts and generated answers for mistakes.\n",
    "\n",
    "a. What types of mistakes do you spot?\n",
    "\n",
    "b. How close are the mistakes to the true answers?\n",
    "\n",
    "c. Why do you think the transformer is making the mistakes it makes?\n",
    "\n",
    "*If your transformer is not making mistakes, print out more prompts/answers or cut the training off a little earlier so the final validation loss is higher.*\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "**Question 5:** Have some fun and prompt your trained transformer with hand crafted addition prompt strings. Try to \"trick\" your transformer:\n",
    "\n",
    "a. using valid prompts (i.e. up to 2 digits per operand). \n",
    "\n",
    "b. using invalid prompts. \n",
    "\n",
    "In both cases document where it does well and where it does not with specific examples.\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "**Question 6:** Add the following code to your GPT's temporal cross entropy loss code: `print(tf.reshape(act_at_correct, (N, T)))` The line of code prints the softmax netAct values produced by the net's output layer from the neuron coding the correct next token. Adapt as necessary to make it print these expected values. Run the code provided below. In some detail, interpret why the netActs are high or low in each case and why that makes sense.\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "**Question 7:** Following up from the previous question, notice how the netActs for the 1st char are small yet nonzero in all cases. Why does this happen and explain why the loss cannot get to exactly 0. \n",
    "\n",
    "**When you are done running Q6 and Q7, comment out the print out in the `loss` method.**\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "**Question 8:** In the cell below, call `generate_sequence` on one of the transformers and plug in a single prompt that yields an incorrect answer based on the above validation set print outs. In your call to `generate_sequence`, set the keyword argument `plot_probs` to `True`. Use the plots of the output layer netActs to explain what is going on when the network predicts the incorrect answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e39c09",
   "metadata": {},
   "source": [
    "**Answer 4:**\n",
    "\n",
    "**Answer 5:**\n",
    "\n",
    "**Answer 6:**\n",
    "\n",
    "**Answer 7:**\n",
    "\n",
    "**Answer 8:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21540606",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca29163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 6 TODO: Replace in the code below:\n",
    "# char2ind_map: with your char-to-int dictionary\n",
    "# gpt_add1: with your trained net\n",
    "\n",
    "prompts = [list('33+1=34.##'), list('33+10=43.#'), list('33+33=66.#')]\n",
    "prompts_int_x, prompts_int_y = make_addition_samples_and_labels(prompts, char2ind_map)\n",
    "prompts_int_x = tf.cast(prompts_int_x, tf.int32)\n",
    "prompts_int_y = tf.cast(prompts_int_y, tf.int32)\n",
    "net_acts = gpt_add1(prompts_int_x)\n",
    "gpt_add1.loss(net_acts, prompts_int_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3c4ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 8.\n",
    "# TODO: modify prompt to one that produces an incorrect output. Change any other variables to suit your conventions\n",
    "prompt = '2+87='\n",
    "answer = gpt_add1.generate_sequence(prompt=prompt,\n",
    "                                length=seq_len,\n",
    "                                char2ind_map=char2ind_map,\n",
    "                                ind2char_map=ind2char_map,\n",
    "                                end_char='.',\n",
    "                                plot_probs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53deffc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs444",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
